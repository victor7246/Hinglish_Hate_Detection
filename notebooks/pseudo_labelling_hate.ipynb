{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kCb0aehNFZC",
    "outputId": "0b816c3d-7f2d-4aa1-fb6a-5856e4be7b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files, drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0uJnM60PPXU",
    "outputId": "540a4f84-78a2-48f0-cf1b-adeb269584a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 2)) (7.1.2)\n",
      "Requirement already satisfied: Sphinx in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.8.5)\n",
      "Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 4)) (3.7.1)\n",
      "Collecting flake8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/ca/3971802ee6251da1abead1a22831d7f4743781e2f743bd266bdd2f46c19b/flake8-3.8.4-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 7.0MB/s \n",
      "\u001b[?25hCollecting python-dotenv\n",
      "  Downloading https://files.pythonhosted.org/packages/32/2e/e4585559237787966aad0f8fd0fc31df1c4c9eb0e62de458c5b6cde954eb/python_dotenv-0.15.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow==2.2.0-rc3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3f/1cf6ba4799a1b88ab387a3e0bb924ec104fa5286508a4d29c903eba36246/tensorflow-2.2.0rc3-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2MB 24kB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 8)) (1.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 9)) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 10)) (4.41.1)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 11)) (0.0)\n",
      "Collecting transformers==2.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 41.5MB/s \n",
      "\u001b[?25hCollecting pytorch-lightning\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/98/86a89dcd54f84582bbf24cb29cd104b966fcf934d92d5dfc626f225015d2/pytorch_lightning-1.1.4-py3-none-any.whl (684kB)\n",
      "\u001b[K     |████████████████████████████████| 686kB 45.9MB/s \n",
      "\u001b[?25hCollecting wandb\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/21/b1d29dca8187d8ebf5a0537b28247c837613bafc2940a840a987586c607f/wandb-0.10.13-py2.py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 44.7MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 45.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 16)) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 17)) (1.4.1)\n",
      "Collecting allennlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/f5/f4dd3424b3ae9dec0a55ae7f7f34ada3ee60e4b10a187d2ba7384c698e09/allennlp-1.3.0-py3-none-any.whl (506kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 48.2MB/s \n",
      "\u001b[?25hCollecting fairseq\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/7b/2c90e007d737f4a2b7cd5066ac3a3d88acb2ce765972a61c308914c95568/fairseq-0.10.2-cp36-cp36m-manylinux1_x86_64.whl (1.7MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7MB 46.6MB/s \n",
      "\u001b[?25hCollecting fastBPE\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
      "Collecting nlpaug\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/6d/34f342ba443ca8a74682962f71c465cfcaaa69e9a437cdcf1756986c110d/nlpaug-1.1.2-py3-none-any.whl (387kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 46.3MB/s \n",
      "\u001b[?25hCollecting textattack\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/82/2f16ef7f22f19b3a49bbcd079dc31e53d362e1ef1299298c3eda05cf2b3a/textattack-0.2.15-py3-none-any.whl (349kB)\n",
      "\u001b[K     |████████████████████████████████| 358kB 57.2MB/s \n",
      "\u001b[?25hCollecting pandas-bokeh\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/9c/4d21c5b6a8e0a4c85c11a03d4f434470f4d2838f8c8b9fe0d94a26008396/pandas_bokeh-0.5.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (2.2.4)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 25)) (3.6.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 26)) (3.2.2)\n",
      "Collecting pyLDAvis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 44.7MB/s \n",
      "\u001b[?25hCollecting captum\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/27/e6d97c600cabc38b860ead2f6be243d819ff3d259bc3195b7d2ed943ba5d/captum-0.3.0-py3-none-any.whl (5.7MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7MB 46.3MB/s \n",
      "\u001b[?25hCollecting eli5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/2f/c85c7d8f8548e460829971785347e14e45fa5c6617da374711dec8cb38cc/eli5-0.10.1-py2.py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 54.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 30)) (1.7.0+cu101)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 53.1MB/s \n",
      "\u001b[?25hCollecting subword_nmt\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 33)) (0.8.3)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (2.4.0)\n",
      "Collecting sklearn_crfsuite\n",
      "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2.11.2)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (0.7.12)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.2.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (51.1.1)\n",
      "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (0.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (20.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 5)) (3.3.0)\n",
      "Collecting pyflakes<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 475kB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (0.36.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (1.32.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (1.1.2)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 55.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 7)) (3.12.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 8)) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 8)) (2.8.1)\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/7f/c7bafbc7db335f6c5194d9b948b0c6fcfe013d14026992150ee749fc5877/boto3-1.16.53-py2.py3-none-any.whl (130kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 56.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 12)) (3.0.12)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 12)) (0.8)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 51.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 12)) (2019.12.20)\n",
      "Collecting PyYAML>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 59.4MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 47.2MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 12.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 14)) (5.4.8)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 14)) (2.3)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Collecting subprocess32>=3.5.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
      "\u001b[?25hCollecting configparser>=3.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/5c/018bf9a5c24343a664deaea70e61f33f53bb1bd3caf193110f827bfd07e2/sentry_sdk-0.19.5-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 52.3MB/s \n",
      "\u001b[?25hCollecting watchdog<0.10.5,>=0.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/10/500580a0987363a0d9e1f3dd5cb1bba94a47e19266c6ce9dfb6cdd455758/watchdog-0.10.4.tar.gz (98kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 13.2MB/s \n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 53.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 16)) (1.0.0)\n",
      "Collecting tensorboardX>=1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 54.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 18)) (3.6.4)\n",
      "Collecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
      "Collecting overrides==3.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 18)) (3.2.5)\n",
      "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 49.7MB/s \n",
      "\u001b[?25hCollecting hydra-core\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/6e/6298e4099ecf7344fb6621e83ec92ba4384699397b2d9b2d17819f869a1d/hydra_core-1.0.5-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 51.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 19)) (0.29.21)\n",
      "Collecting sacrebleu>=1.4.12\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 19)) (1.14.4)\n",
      "Collecting word2number\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (0.5.3)\n",
      "Collecting bert-score>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/f9/7bd03a70ccb4e3f6a334f5f5dc8871d76de10144d277d23f730b08cf2aaf/bert_score-0.3.7-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (1.7.1)\n",
      "Collecting terminaltables\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
      "Collecting flair==0.6.1.post1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/49/a812ed93088ba9519cbb40eb9f52341694b31cfa126bfddcd9db3761f3ac/flair-0.6.1.post1-py3-none-any.whl (337kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 49.7MB/s \n",
      "\u001b[?25hCollecting lru-dict\n",
      "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
      "Collecting datasets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/78/5873ac1e27bf25a2cbf3447d6704edd3136b1b3ff0eb3bfab38a45d2a1ff/datasets-1.2.0-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 54.7MB/s \n",
      "\u001b[?25hCollecting num2words\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 14.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (8.6.0)\n",
      "Collecting lemminflect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/c5/62e8dd0b6cbfea212cf55a2338838d85a819dbda9462ba53a415dcf19b86/lemminflect-0.2.1-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 51.6MB/s \n",
      "\u001b[?25hCollecting language-tool-python\n",
      "  Downloading https://files.pythonhosted.org/packages/49/81/945fec190c623cd3f08860216fd1a107541677b742fa2df4a9b8fc6c4b24/language_tool_python-2.5.1-py3-none-any.whl\n",
      "Requirement already satisfied: bokeh>=2.0 in /usr/local/lib/python3.6/dist-packages (from pandas-bokeh->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 23)) (2.1.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (2.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (0.8.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 24)) (1.0.5)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 25)) (4.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 26)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 26)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 26)) (0.10.0)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 27)) (2.7.2)\n",
      "Collecting funcy\n",
      "  Downloading https://files.pythonhosted.org/packages/66/89/479de0afbbfb98d1c4b887936808764627300208bb771fcd823403645a36/funcy-1.15-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 29)) (0.8.7)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 29)) (0.10.1)\n",
      "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 29)) (20.3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 30)) (3.7.4.3)\n",
      "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 33)) (2.7.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (1.17.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (3.3.3)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 58.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.1.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->Sphinx->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 5)) (3.4.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
      "\u001b[?25hCollecting botocore<1.20.0,>=1.19.53\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/89/f2511851beb7d687ac45638a0e62b5cdeb0abca572ac7d3d250d6f4c888f/botocore-1.19.53-py2.py3-none-any.whl (7.2MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2MB 53.3MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 48.0MB/s \n",
      "\u001b[?25hCollecting pathtools>=0.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 18)) (0.7.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 18)) (1.4.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 18)) (1.10.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 19)) (4.1.1)\n",
      "Collecting omegaconf<2.1,>=2.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 59.0MB/s \n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 19)) (2.20)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 44.4MB/s \n",
      "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting janome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7MB 63.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (3.6.4)\n",
      "Collecting mpld3==0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 49.2MB/s \n",
      "\u001b[?25hCollecting ftfy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (0.1.2)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (4.2.6)\n",
      "Collecting pyarrow>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7MB 208kB/s \n",
      "\u001b[?25hCollecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 58.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (0.3.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (0.70.11.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (0.6.2)\n",
      "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh>=2.0->pandas-bokeh->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 23)) (7.0.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh>=2.0->pandas-bokeh->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 23)) (5.1.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (4.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (1.3.0)\n",
      "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 60.2MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 59.0MB/s \n",
      "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (0.2.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (2.5)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (3.11.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 34)) (3.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.6.1.post1->textattack->-r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt (line 22)) (4.4.2)\n",
      "Building wheels for collected packages: fastBPE, pyLDAvis, sacremoses, PyYAML, future, subprocess32, watchdog, overrides, jsonnet, word2number, terminaltables, lru-dict, pathtools, antlr4-python3-runtime, segtok, langdetect, sqlitedict, mpld3, ftfy, idna-ssl\n",
      "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481537 sha256=f5dff9fc3f2958dd079a7776133a23ca9faa969ce441f6dbf37bb24b8f82bb68\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
      "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=a212ee759a679f83ebeef3e9a24ac7c61bc0dfcea492cf879038a0a65bc9d45b\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=9de554a10f48772ae47228a4aeb6c882c401c3f611b6c832c1433102de698561\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=2f280cda23fb3da171a89155bae57d860278c8a08215ad187040cd3adc8abcc1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=4475728bca898a8495a3705804b6c7fdd101acec50f8e15a6312e01b5416d526\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6490 sha256=63f79ed34f6d400a6e309f847ba76d5841beb76e4bbf6548707ba2937e86f287\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
      "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for watchdog: filename=watchdog-0.10.4-cp36-none-any.whl size=74842 sha256=56f61aa0d92b919b20df5256daaa45b28994b996a6328a73f08a743eb15ee56a\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/11/04/5160b8815b0cc7cf574bdc6d053e510169ec264c8791b4ec3a\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10175 sha256=4a810ce4429de720a214069d74056430ad8c8bf939cdbd8b2955bb6c93698e87\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387940 sha256=ea42a6c280e79858890531780984d7c938e75d6803c36df07d4b8c3c687e51bd\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=581e483bdf37177caa6e90a2083f161f14cbeb4dea6cc8aef399fa16129075a0\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=eaaf363563fb88e7da0b38334c0a214620f68c16883916a8f82ca7540331ca81\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
      "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lru-dict: filename=lru_dict-1.1.6-cp36-cp36m-linux_x86_64.whl size=25867 sha256=359158583ed597372b12bab17e5fb6d6ed116aaff7e15b80d6e598b1f6d79471\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/ef/06/fbdd555907a7d438fb33e4c8675f771ff1cf41917284c51ebf\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=b71f84d5254708852758a2c606204e967cf34c7c87aea0dcde6a57805b4b8f1b\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141231 sha256=3e567143809ee1bd820ac6962ac7efd2d2f9d7a90a1d8c244d25fafadb20295f\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25019 sha256=0c471477f6cfa35824c359bf8940f2699ab6c640078b3b5840d4aeb769f22da6\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993194 sha256=08d7a402bb9858f54498d59e23e0f0337a1c3b848da89ffc335b9e3edd3b4af3\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14376 sha256=450fd376ca660a0e4de012ebd5d290ab403be1bfdfabfe76b5959a76fe84f2e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116678 sha256=61e1d634f716d07e905b24c79c8570cccb951e003db8e3b59105d2f943d66607\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=f17ac0d7424efd69b9957c5773516f7b558d2b36cc66d5f4cd1aaccb630d61bf\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=ccb3f1a98731131451f07f5a1b4b7ec514e2877992be103e2ab81a17803daddb\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
      "Successfully built fastBPE pyLDAvis sacremoses PyYAML future subprocess32 watchdog overrides jsonnet word2number terminaltables lru-dict pathtools antlr4-python3-runtime segtok langdetect sqlitedict mpld3 ftfy idna-ssl\n",
      "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: botocore 1.19.53 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: transformers 2.8.0 has requirement tokenizers==0.5.2, but you'll have tokenizers 0.8.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: allennlp 1.3.0 has requirement transformers<4.1,>=4.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: bert-score 0.3.7 has requirement transformers>=3.0.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: konoha 4.6.2 has requirement overrides==3.0.0, but you'll have overrides 3.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: flair 0.6.1.post1 has requirement transformers>=3.0.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.15 has requirement numpy<1.19.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: textattack 0.2.15 has requirement transformers>=3.3.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyflakes, mccabe, pycodestyle, flake8, python-dotenv, tensorflow-estimator, tensorflow, tokenizers, jmespath, botocore, s3transfer, boto3, sacremoses, sentencepiece, transformers, PyYAML, future, idna-ssl, async-timeout, multidict, yarl, aiohttp, fsspec, pytorch-lightning, shortuuid, docker-pycreds, subprocess32, configparser, sentry-sdk, pathtools, watchdog, smmap, gitdb, GitPython, wandb, tensorboardX, jsonpickle, overrides, jsonnet, allennlp, omegaconf, antlr4-python3-runtime, hydra-core, portalocker, sacrebleu, fairseq, fastBPE, nlpaug, word2number, bert-score, terminaltables, segtok, bpemb, langdetect, sqlitedict, janome, mpld3, ftfy, konoha, deprecated, flair, lru-dict, pyarrow, xxhash, datasets, num2words, lemminflect, language-tool-python, textattack, pandas-bokeh, funcy, pyLDAvis, captum, eli5, subword-nmt, python-crfsuite, sklearn-crfsuite\n",
      "  Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Found existing installation: tensorflow 2.4.0\n",
      "    Uninstalling tensorflow-2.4.0:\n",
      "      Successfully uninstalled tensorflow-2.4.0\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: pyarrow 0.14.1\n",
      "    Uninstalling pyarrow-0.14.1:\n",
      "      Successfully uninstalled pyarrow-0.14.1\n",
      "Successfully installed GitPython-3.1.12 PyYAML-5.3.1 aiohttp-3.7.3 allennlp-1.3.0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 bert-score-0.3.7 boto3-1.16.53 botocore-1.19.53 bpemb-0.3.2 captum-0.3.0 configparser-5.0.1 datasets-1.2.0 deprecated-1.2.10 docker-pycreds-0.4.0 eli5-0.10.1 fairseq-0.10.2 fastBPE-0.1.0 flair-0.6.1.post1 flake8-3.8.4 fsspec-0.8.5 ftfy-5.8 funcy-1.15 future-0.18.2 gitdb-4.0.5 hydra-core-1.0.5 idna-ssl-1.1.0 janome-0.4.1 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 konoha-4.6.2 langdetect-1.0.8 language-tool-python-2.5.1 lemminflect-0.2.1 lru-dict-1.1.6 mccabe-0.6.1 mpld3-0.3 multidict-5.1.0 nlpaug-1.1.2 num2words-0.5.10 omegaconf-2.0.5 overrides-3.1.0 pandas-bokeh-0.5.2 pathtools-0.1.2 portalocker-2.0.0 pyLDAvis-2.1.2 pyarrow-2.0.0 pycodestyle-2.6.0 pyflakes-2.2.0 python-crfsuite-0.9.7 python-dotenv-0.15.0 pytorch-lightning-1.1.4 s3transfer-0.3.4 sacrebleu-1.4.14 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.95 sentry-sdk-0.19.5 shortuuid-1.0.1 sklearn-crfsuite-0.3.6 smmap-3.0.4 sqlitedict-1.7.0 subprocess32-3.5.4 subword-nmt-0.3.7 tensorboardX-2.1 tensorflow-2.2.0rc3 tensorflow-estimator-2.2.0 terminaltables-3.1.0 textattack-0.2.15 tokenizers-0.8.1 transformers-2.8.0 wandb-0.10.13 watchdog-0.10.4 word2number-1.1 xxhash-2.0.0 yarl-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWAK8Y7Gk44m"
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1CqQIcXPSkD",
    "outputId": "6ef32151-40a0-474a-87ee-e58dcb9271b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "#!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-IqVp0DPLFa",
    "outputId": "dce8c9de-8650-4a0f-b753-3b9d43994c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), './drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), './drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/'))\n",
    "    \n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    load_dotenv(find_dotenv())\n",
    "    wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    from wandb.keras import WandbCallback\n",
    "    _has_wandb = True\n",
    "except:\n",
    "    _has_wandb = False\n",
    "\n",
    "import tokenizers\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoConfig, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from src import data, models\n",
    "\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWQ_RjDKPF_9",
    "outputId": "2ebab752-bd3f-4773-a2ba-6249c3957fde"
   },
   "outputs": [],
   "source": [
    "print (_has_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKb2wFKVNFZH"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(prog='Trainer',conflict_handler='resolve')\n",
    "\n",
    "#parser.add_argument('--train_data', type=str, default='./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/raw/Hate-speech-dataset/hate_speech.tsv', required=False,\n",
    "#                    help='train data')\n",
    "parser.add_argument('--train_data', type=str, default='../data/raw/Hate-speech-dataset/hate_speech.tsv', required=False,\n",
    "                    help='train data')\n",
    "\n",
    "parser.add_argument('--val_data', type=str, default=None, required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=50, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--max_char_len', type=int, default=200, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--max_word_char_len', type=int, default=20, required=False,\n",
    "                    help='maximum length of text')\n",
    "\n",
    "parser.add_argument('--emb_dim', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--n_layers', type=int, default=2, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--n_units', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=500, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.001, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--early_stopping_rounds', type=int, default=50, required=False,\n",
    "                    help='number of epochs for early stopping')\n",
    "parser.add_argument('--lr_schedule_round', type=int, default=30, required=False,\n",
    "                    help='number of epochs for learning rate scheduling')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=32, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "#parser.add_argument('--model_save_path', type=str, default='./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/', required=False,\n",
    "#                    help='seed')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/hate_detection/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=True, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjTSfc1xNFZJ"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pBa4QUENFZN"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/raw/Hate-speech-dataset/hate_speech.tsv', \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'hate'\n",
    "\n",
    "hate_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "hate_val_df = df.iloc[val_index]\n",
    "hate_test_df = df.iloc[test_index]\n",
    "\n",
    "df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/StanceDetection_CodeMixed/data.txt', \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'stance'\n",
    "stance_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "stance_val_df = df.iloc[val_index]\n",
    "stance_test_df = df.iloc[test_index]\n",
    "\n",
    "df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/SarcasmDetection_CodeMixed/data.txt', \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'sarcasm'\n",
    "\n",
    "sarcasm_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "sarcasm_val_df = df.iloc[val_index]\n",
    "sarcasm_test_df = df.iloc[test_index]\n",
    "\n",
    "df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/humor-detection-corpus/data.txt', \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'humor'\n",
    "\n",
    "humor_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "humor_val_df = df.iloc[val_index]\n",
    "humor_test_df = df.iloc[test_index]\n",
    "\n",
    "df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/Aggression_dataset/train.txt', \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df = pd.concat([df,pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/Aggression_dataset/val.txt', \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])],axis=0)\n",
    "\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "aggression_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "aggression_val_df = df.iloc[val_index]\n",
    "aggression_test_df = df.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrfukC0aNFZW",
    "outputId": "71577480-9f40-44be-b66c-46e6a77af157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3662, 2) (458, 2) (458, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([hate_train_df,hate_train_df,sarcasm_train_df,humor_train_df,aggression_train_df],axis=0)\n",
    "val_df = pd.concat([hate_val_df,hate_val_df,sarcasm_val_df,humor_val_df,aggression_val_df],axis=0)\n",
    "val_df = pd.concat([hate_test_df,hate_test_df,sarcasm_test_df,humor_test_df,aggression_test_df],axis=0)\n",
    "\n",
    "print (train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "GWR12E5dNFZZ",
    "outputId": "db3dd43e-a436-4b60-911a-d197b0b7c50e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knowing ki Vikas kitna samjhata hai Priyanka aur Itch Guard Luv ko, usne bola tha Ben wali baat me ab Sallu ne bhi agree kiya!</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am Muhajir .. Aur mere lye sab se Pehly Pakistan he .. agr 10 lakh Altaf Jese leaders bh is zameen ki behurmati kren un sbko sar e aam phansi Deni chahye .. Proud to be a #Muhajir and #Pakistani</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doctor  sab sahi me ke PhD (in hate politics) wale. Bhai padhe likhe ho fir kyu ye sab baate karte ho. Tum bas bowling  khelo, aur maje lo. pic.twitter.com/fk1qUbQstw</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poore Desh me Patel OBC me aate Hain sirf gujrat Ko chor kar may be, ye manuwadiyon bramanwadi kabhi aapko aarackchan nahi denge ye to jis OBC Ko Mila hai usse bhi nafrat karte hain ye khoon aur chamdi ka frak karne waale bharmhanwadi kisi ke sage nahi hain</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sarkar banne ke bad Hindu hit me ek bhi faisla Jo bjp ke dwara liya gaya ho,bjp ko  gay,gobar,mandir,masjid aur nafrat faila kar vot chahiye</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                text category\n",
       "0  Knowing ki Vikas kitna samjhata hai Priyanka aur Itch Guard Luv ko, usne bola tha Ben wali baat me ab Sallu ne bhi agree kiya!                                                                                                                                     no     \n",
       "1  I am Muhajir .. Aur mere lye sab se Pehly Pakistan he .. agr 10 lakh Altaf Jese leaders bh is zameen ki behurmati kren un sbko sar e aam phansi Deni chahye .. Proud to be a #Muhajir and #Pakistani                                                               no     \n",
       "2  Doctor  sab sahi me ke PhD (in hate politics) wale. Bhai padhe likhe ho fir kyu ye sab baate karte ho. Tum bas bowling  khelo, aur maje lo. pic.twitter.com/fk1qUbQstw                                                                                             no     \n",
       "3  Poore Desh me Patel OBC me aate Hain sirf gujrat Ko chor kar may be, ye manuwadiyon bramanwadi kabhi aapko aarackchan nahi denge ye to jis OBC Ko Mila hai usse bhi nafrat karte hain ye khoon aur chamdi ka frak karne waale bharmhanwadi kisi ke sage nahi hain  no     \n",
       "4  Sarkar banne ke bad Hindu hit me ek bhi faisla Jo bjp ke dwara liya gaya ho,bjp ko  gay,gobar,mandir,masjid aur nafrat faila kar vot chahiye                                                                                                                       yes    "
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JFvdZlgPF_9",
    "outputId": "939b5174-07a0-4fb0-adcf-9051fdf6ad8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "train_df.text = train_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "val_df.text = val_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "test_df.text = test_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "\n",
    "train_df = train_df[train_df.text != '']\n",
    "val_df = val_df[val_df.text != '']\n",
    "test_df = test_df[test_df.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76PsUvCk3rUZ"
   },
   "outputs": [],
   "source": [
    "hate_train_df.text = hate_train_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "hate_val_df.text = hate_val_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "hate_test_df.text = hate_test_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "\n",
    "hate_train_df = hate_train_df[hate_train_df.text != '']\n",
    "hate_val_df = hate_val_df[hate_val_df.text != '']\n",
    "hate_test_df = hate_test_df[hate_test_df.text != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecLj6RWuVfdt",
    "outputId": "1a4831a6-335d-42b5-dd9a-732f2cf4ea0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3661.000000\n",
       "mean     102.369571 \n",
       "std      53.175251  \n",
       "min      6.000000   \n",
       "25%      63.000000  \n",
       "50%      99.000000  \n",
       "75%      130.000000 \n",
       "max      313.000000 \n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text.apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oAyiM1jPF_9",
    "outputId": "051d8dd7-1243-4a7b-dfb4-ef2bde3f92e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3661.000000\n",
       "mean     19.663207  \n",
       "std      10.279373  \n",
       "min      1.000000   \n",
       "25%      12.000000  \n",
       "50%      19.000000  \n",
       "75%      25.000000  \n",
       "max      64.000000  \n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text.apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7YDv6wZNFZc"
   },
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5_NT3X9cuiO",
    "outputId": "36176967-5313-4cec-ea1a-becbc9916b17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     2316\n",
       "yes    1342\n",
       "on     2   \n",
       "n      1   \n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_train_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihJokEqR3rUa"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.category.str.contains('yes|no')]\n",
    "val_df = val_df[val_df.category.str.contains('yes|no')]\n",
    "test_df = test_df[test_df.category.str.contains('yes|no')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HogRodQIkh1O"
   },
   "outputs": [],
   "source": [
    "hate_train_df = hate_train_df[hate_train_df.category.str.contains('yes|no')]\n",
    "hate_val_df = hate_val_df[hate_val_df.category.str.contains('yes|no')]\n",
    "hate_test_df = hate_test_df[hate_test_df.category.str.contains('yes|no')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SQFNK1zLdyT7",
    "outputId": "f7ea9eb9-0629-4ac6-a8ad-9ba2a1525364"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch7LVidpBNtE"
   },
   "outputs": [],
   "source": [
    "hate_train_df.category, label2idx = data.data_utils.convert_categorical_label_to_int(hate_train_df.category.values, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))\n",
    "\n",
    "hate_val_df.category, _ = data.data_utils.convert_categorical_label_to_int(hate_val_df.category.values, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))\n",
    "\n",
    "hate_test_df.category, _ = data.data_utils.convert_categorical_label_to_int(hate_test_df.category.values, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "1o2K5yrlNFZv",
    "outputId": "e60e9d3a-0870-4515-82ac-0cd3faa4e1dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knowing ki vikas kitna samjhata hai priyanka aur itch guard luv ko, usne bola tha ben wali baat me ab sallu ne bhi agree kiya!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i am muhajir .. aur mere lye sab se pehly pakistan he .. agr  lakh altaf jese leaders bh is zameen ki behurmati kren un sbko sar e aam phansi deni chahye .. proud to be a  and</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctor  sab sahi me ke phd (in hate politics) wale. bhai padhe likhe ho fir kyu ye sab baate karte ho. tum bas bowling  khelo, aur maje lo. pic.twitter.com/fkqubqstw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poore desh me patel obc me aate hain sirf gujrat ko chor kar may be, ye manuwadiyon bramanwadi kabhi aapko aarackchan nahi denge ye to jis obc ko mila hai usse bhi nafrat karte hain ye khoon aur chamdi ka frak karne waale bharmhanwadi kisi ke sage nahi hain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sarkar banne ke bad hindu hit me ek bhi faisla jo bjp ke dwara liya gaya ho,bjp ko  gay,gobar,mandir,masjid aur nafrat faila kar vot chahiye</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                text  category\n",
       "0  knowing ki vikas kitna samjhata hai priyanka aur itch guard luv ko, usne bola tha ben wali baat me ab sallu ne bhi agree kiya!                                                                                                                                     0       \n",
       "1  i am muhajir .. aur mere lye sab se pehly pakistan he .. agr  lakh altaf jese leaders bh is zameen ki behurmati kren un sbko sar e aam phansi deni chahye .. proud to be a  and                                                                                    0       \n",
       "2  doctor  sab sahi me ke phd (in hate politics) wale. bhai padhe likhe ho fir kyu ye sab baate karte ho. tum bas bowling  khelo, aur maje lo. pic.twitter.com/fkqubqstw                                                                                              0       \n",
       "3  poore desh me patel obc me aate hain sirf gujrat ko chor kar may be, ye manuwadiyon bramanwadi kabhi aapko aarackchan nahi denge ye to jis obc ko mila hai usse bhi nafrat karte hain ye khoon aur chamdi ka frak karne waale bharmhanwadi kisi ke sage nahi hain  0       \n",
       "4  sarkar banne ke bad hindu hit me ek bhi faisla jo bjp ke dwara liya gaya ho,bjp ko  gay,gobar,mandir,masjid aur nafrat faila kar vot chahiye                                                                                                                       1       "
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBiGShWtBRKW",
    "outputId": "645294b1-0a48-4f71-8488-77c05e629180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 0, 'yes': 1}\n"
     ]
    }
   ],
   "source": [
    "print (label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DPcQeSiBSaV"
   },
   "outputs": [],
   "source": [
    "idx2label = {i:w for (w,i) in label2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isoI88xCPF_-"
   },
   "source": [
    "### Learn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4zgghyWPF_-"
   },
   "outputs": [],
   "source": [
    "data.custom_tokenizers.custom_wp_tokenizer(hate_train_df.text.values, args.model_save_path, args.model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBWTdHbAPF_-"
   },
   "outputs": [],
   "source": [
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000, split=' ',oov_token=1)\n",
    "char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, split='',oov_token=1)\n",
    "\n",
    "word_tokenizer.fit_on_texts(hate_train_df.text.values)\n",
    "char_tokenizer.fit_on_texts(hate_train_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GioC_GWuNFZ9",
    "outputId": "3c06df7c-8fc8-426e-e568-25c3c8635a81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3658/3658 [00:01<00:00, 2437.81it/s]\n",
      "100%|██████████| 3658/3658 [00:02<00:00, 1482.06it/s]\n",
      "100%|██████████| 457/457 [00:00<00:00, 2202.44it/s]\n",
      "100%|██████████| 457/457 [00:00<00:00, 1433.24it/s]\n",
      "100%|██████████| 458/458 [00:00<00:00, 2398.98it/s]\n",
      "100%|██████████| 458/458 [00:00<00:00, 1387.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3658, 200) (3658, 50, 20) (3658, 50) (3658, 200) (3658, 18201) (3658, 2)\n",
      "(457, 200) (457, 50, 20) (457, 50) (457, 200) (457, 18201) (457, 2)\n",
      "(458, 200) (458, 50, 20) (458, 50) (458, 200) (458, 18201) (458, 2)\n"
     ]
    }
   ],
   "source": [
    "transformer_train_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(train_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_train_inputs = word_tokenizer.texts_to_sequences(train_df.text.values)\n",
    "word_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_train_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_train_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(train_df.text.values)])\n",
    "\n",
    "char_train_inputs = char_tokenizer.texts_to_sequences(train_df.text.values)\n",
    "char_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_train_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "train_outputs = data.data_utils.compute_output_arrays(train_df, 'category')\n",
    "\n",
    "transformer_val_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(hate_val_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_val_inputs = word_tokenizer.texts_to_sequences(hate_val_df.text.values)\n",
    "word_val_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_val_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_val_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(hate_val_df.text.values)])\n",
    "\n",
    "char_val_inputs = char_tokenizer.texts_to_sequences(hate_val_df.text.values)\n",
    "char_val_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_val_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "val_outputs = data.data_utils.compute_output_arrays(hate_val_df, 'category')\n",
    "\n",
    "transformer_test_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(test_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_test_inputs = word_tokenizer.texts_to_sequences(test_df.text.values)\n",
    "word_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_test_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_test_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(test_df.text.values)])\n",
    "\n",
    "char_test_inputs = char_tokenizer.texts_to_sequences(test_df.text.values)\n",
    "char_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_test_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "test_outputs = data.data_utils.compute_output_arrays(test_df, 'category')\n",
    "\n",
    "train_outputs = tf.keras.utils.to_categorical(train_outputs, \\\n",
    "                                                    num_classes=train_df.category.nunique())\n",
    "val_outputs = tf.keras.utils.to_categorical(val_outputs, \\\n",
    "                                                    num_classes=train_df.category.nunique())\n",
    "test_outputs = tf.keras.utils.to_categorical(test_outputs, \\\n",
    "                                                    num_classes=train_df.category.nunique())\n",
    "\n",
    "print (transformer_train_inputs.shape, subword_train_inputs.shape, word_train_inputs.shape, char_train_inputs.shape, \\\n",
    "       train_outputs.shape)\n",
    "print (transformer_val_inputs.shape, subword_val_inputs.shape, word_val_inputs.shape, char_val_inputs.shape, \\\n",
    "       val_outputs.shape)\n",
    "print (transformer_test_inputs.shape, subword_test_inputs.shape, word_test_inputs.shape, char_test_inputs.shape, \\\n",
    "       test_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gSp7NirPF_-"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8cHPep9PF_-"
   },
   "outputs": [],
   "source": [
    "n_words = len(word_tokenizer.word_index)+1\n",
    "n_chars = len(char_tokenizer.word_index)+1\n",
    "n_subwords = tokenizer.vocab_size\n",
    "tfidf_shape = train_tfidf.shape[1]\n",
    "n_out = train_df.category.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHYdrtMGlDe7"
   },
   "outputs": [],
   "source": [
    "from src.models.models import *\n",
    "\n",
    "#all_models = {HAN.__name__: HAN, \\\n",
    "#              CMSA.__name__: CMSA, CS_ELMO_without_words.__name__: CS_ELMO_without_words}\n",
    "\n",
    "all_models = {HAN.__name__: HAN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1y38uBX47Pe"
   },
   "outputs": [],
   "source": [
    "_has_wandb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TQGocn-PF_-",
    "outputId": "88caa3da-7fc5-4559-eed0-e0b26fc4dd17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                     config  ...  macro_f1\n",
      "0   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CS_ELMO_without_words', 'loss': 'ce', 'use_features': False}     ...  0.938759\n",
      "1   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CS_ELMO_without_words', 'loss': 'focal', 'use_features': False}  ...  0.918659\n",
      "2   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HAN', 'loss': 'ce', 'use_features': False}                       ...  0.921351\n",
      "3   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HAN', 'loss': 'focal', 'use_features': False}                    ...  0.930697\n",
      "4   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CMSA', 'loss': 'ce', 'use_features': False}                      ...  0.921050\n",
      "5   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CMSA', 'loss': 'focal', 'use_features': False}                   ...  0.927987\n",
      "6   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'ce', 'use_features': True}                        ...  0.919118\n",
      "7   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'ce', 'use_features': False}                       ...  0.911536\n",
      "8   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'focal', 'use_features': True}                     ...  0.916332\n",
      "9   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'focal', 'use_features': False}                    ...  0.919455\n",
      "10  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'ce', 'use_features': True}                  ...  0.921225\n",
      "11  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'ce', 'use_features': False}                 ...  0.918065\n",
      "12  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'focal', 'use_features': True}               ...  0.921225\n",
      "13  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'focal', 'use_features': False}              ...  0.910721\n",
      "14  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'ce', 'use_features': True}          ...  0.934212\n",
      "15  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'ce', 'use_features': False}         ...  0.566288\n",
      "16  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'focal', 'use_features': True}       ...  0.923010\n",
      "17  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'focal', 'use_features': False}      ...  0.715545\n",
      "18  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer_without_words', 'loss': 'ce', 'use_features': True}    ...  0.911102\n",
      "19  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': True}                ...  0.926701\n",
      "20  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': False}               ...  0.915973\n",
      "21  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': True}             ...  0.914612\n",
      "22  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': False}            ...  0.911202\n",
      "\n",
      "[23 rows x 3 columns]\n",
      "Running Transformer with features for ce loss\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding (T (None, 200, 128)     687360      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 200, 128)     99584       token_and_position_embedding[0][0\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_1 (Transforme (None, 200, 128)     99584       transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           transformer_block_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 18201)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 18329)        0           dropout_4[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          2346240     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 50, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            258         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,233,026\n",
      "Trainable params: 3,233,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "\n",
      "Score 0.6593078902436091. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_with_features.h5.\n",
      "115/115 - 5s - loss: 0.6791 - accuracy: 0.6096 - f1_keras: 0.4530 - val_loss: 0.4587 - val_accuracy: 0.7965 - val_f1_keras: 0.6058 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "\n",
      "Score 0.8157623215737722. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_with_features.h5.\n",
      "115/115 - 4s - loss: 0.4880 - accuracy: 0.7840 - f1_keras: 0.7254 - val_loss: 0.3158 - val_accuracy: 0.8709 - val_f1_keras: 0.7737 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "\n",
      "Score 0.8973931745131405. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_with_features.h5.\n",
      "115/115 - 4s - loss: 0.3223 - accuracy: 0.8852 - f1_keras: 0.8675 - val_loss: 0.2329 - val_accuracy: 0.9168 - val_f1_keras: 0.8611 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "\n",
      "Score 0.9126999287984301. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_with_features.h5.\n",
      "115/115 - 4s - loss: 0.1811 - accuracy: 0.9516 - f1_keras: 0.9460 - val_loss: 0.2126 - val_accuracy: 0.9278 - val_f1_keras: 0.8827 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "\n",
      "Score 0.924394970693893. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_with_features.h5.\n",
      "115/115 - 4s - loss: 0.0850 - accuracy: 0.9869 - f1_keras: 0.9860 - val_loss: 0.1677 - val_accuracy: 0.9387 - val_f1_keras: 0.8950 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "\n",
      "Score 0.9183481744041959. Model not saved.\n",
      "115/115 - 4s - loss: 0.0409 - accuracy: 0.9964 - f1_keras: 0.9961 - val_loss: 0.2088 - val_accuracy: 0.9322 - val_f1_keras: 0.8874 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "\n",
      "Score 0.9155307056051013. Model not saved.\n",
      "115/115 - 4s - loss: 0.0238 - accuracy: 0.9986 - f1_keras: 0.9983 - val_loss: 0.1940 - val_accuracy: 0.9300 - val_f1_keras: 0.8858 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "\n",
      "Score 0.9204608744082428. Model not saved.\n",
      "115/115 - 4s - loss: 0.0126 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.2074 - val_accuracy: 0.9344 - val_f1_keras: 0.8899 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0097 - accuracy: 0.9986 - f1_keras: 0.9984 - val_loss: 0.2351 - val_accuracy: 0.9234 - val_f1_keras: 0.8673 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "\n",
      "Score 0.8909976829766935. Model not saved.\n",
      "115/115 - 4s - loss: 0.0099 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.2775 - val_accuracy: 0.9081 - val_f1_keras: 0.8428 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "\n",
      "Score 0.9351956891661941. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_with_features.h5.\n",
      "115/115 - 5s - loss: 0.0094 - accuracy: 0.9986 - f1_keras: 0.9986 - val_loss: 0.2078 - val_accuracy: 0.9475 - val_f1_keras: 0.8952 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "\n",
      "Score 0.9143914204093102. Model not saved.\n",
      "115/115 - 4s - loss: 0.0074 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.2406 - val_accuracy: 0.9300 - val_f1_keras: 0.8652 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "\n",
      "Score 0.9045578980046849. Model not saved.\n",
      "115/115 - 4s - loss: 0.0068 - accuracy: 0.9989 - f1_keras: 0.9989 - val_loss: 0.3244 - val_accuracy: 0.9190 - val_f1_keras: 0.8552 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "\n",
      "Score 0.9215097513162648. Model not saved.\n",
      "115/115 - 4s - loss: 0.0054 - accuracy: 0.9995 - f1_keras: 0.9993 - val_loss: 0.2453 - val_accuracy: 0.9365 - val_f1_keras: 0.8834 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "\n",
      "Score 0.9225934946528593. Model not saved.\n",
      "115/115 - 4s - loss: 0.0076 - accuracy: 0.9989 - f1_keras: 0.9985 - val_loss: 0.2272 - val_accuracy: 0.9365 - val_f1_keras: 0.8820 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "\n",
      "Score 0.9272657608535471. Model not saved.\n",
      "115/115 - 4s - loss: 0.0099 - accuracy: 0.9984 - f1_keras: 0.9982 - val_loss: 0.2266 - val_accuracy: 0.9409 - val_f1_keras: 0.8864 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "\n",
      "Score 0.904972043805739. Model not saved.\n",
      "115/115 - 4s - loss: 0.0046 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2976 - val_accuracy: 0.9212 - val_f1_keras: 0.8563 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "\n",
      "Score 0.9053830227743271. Model not saved.\n",
      "115/115 - 4s - loss: 0.0109 - accuracy: 0.9978 - f1_keras: 0.9975 - val_loss: 0.2864 - val_accuracy: 0.9212 - val_f1_keras: 0.8549 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "\n",
      "Score 0.9143914204093102. Model not saved.\n",
      "115/115 - 4s - loss: 0.0052 - accuracy: 0.9995 - f1_keras: 0.9992 - val_loss: 0.2668 - val_accuracy: 0.9300 - val_f1_keras: 0.8644 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "\n",
      "Score 0.9106395215090868. Model not saved.\n",
      "115/115 - 4s - loss: 0.0078 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3012 - val_accuracy: 0.9256 - val_f1_keras: 0.8604 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 0.0032 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.3258 - val_accuracy: 0.9278 - val_f1_keras: 0.8631 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "\n",
      "Score 0.9126999287984301. Model not saved.\n",
      "115/115 - 4s - loss: 0.0050 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.2943 - val_accuracy: 0.9278 - val_f1_keras: 0.8627 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "\n",
      "Score 0.9240359042553192. Model not saved.\n",
      "115/115 - 4s - loss: 0.0068 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.2416 - val_accuracy: 0.9387 - val_f1_keras: 0.8860 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "\n",
      "Score 0.9168764265136448. Model not saved.\n",
      "115/115 - 4s - loss: 0.0078 - accuracy: 0.9986 - f1_keras: 0.9983 - val_loss: 0.2909 - val_accuracy: 0.9322 - val_f1_keras: 0.8764 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "\n",
      "Score 0.8978314330427007. Model not saved.\n",
      "115/115 - 4s - loss: 0.0065 - accuracy: 0.9986 - f1_keras: 0.9984 - val_loss: 0.3755 - val_accuracy: 0.9125 - val_f1_keras: 0.8435 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "\n",
      "Score 0.897432444564144. Model not saved.\n",
      "115/115 - 4s - loss: 0.0060 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.4615 - val_accuracy: 0.9125 - val_f1_keras: 0.8427 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "\n",
      "Score 0.9082081800600277. Model not saved.\n",
      "115/115 - 4s - loss: 0.0049 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3644 - val_accuracy: 0.9234 - val_f1_keras: 0.8579 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "\n",
      "Score 0.9141784037558685. Model not saved.\n",
      "115/115 - 4s - loss: 0.0038 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.3236 - val_accuracy: 0.9278 - val_f1_keras: 0.8619 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "\n",
      "Score 0.9025608223359367. Model not saved.\n",
      "115/115 - 4s - loss: 0.0057 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3744 - val_accuracy: 0.9168 - val_f1_keras: 0.8521 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "\n",
      "Score 0.9093505772206134. Model not saved.\n",
      "115/115 - 4s - loss: 0.0093 - accuracy: 0.9986 - f1_keras: 0.9984 - val_loss: 0.2972 - val_accuracy: 0.9234 - val_f1_keras: 0.8569 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "\n",
      "Score 0.9176265371980115. Model not saved.\n",
      "115/115 - 4s - loss: 0.0057 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.2509 - val_accuracy: 0.9322 - val_f1_keras: 0.8799 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "\n",
      "Score 0.8884673411430599. Model not saved.\n",
      "115/115 - 4s - loss: 0.0082 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.5108 - val_accuracy: 0.9037 - val_f1_keras: 0.8352 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 0.0062 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2872 - val_accuracy: 0.9278 - val_f1_keras: 0.8718 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "\n",
      "Score 0.8977176863526022. Model not saved.\n",
      "115/115 - 4s - loss: 0.0028 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.3358 - val_accuracy: 0.9147 - val_f1_keras: 0.8486 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Score 0.9135942522215921. Model not saved.\n",
      "115/115 - 4s - loss: 0.0088 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.2794 - val_accuracy: 0.9300 - val_f1_keras: 0.8743 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "\n",
      "Score 0.9098556576626753. Model not saved.\n",
      "115/115 - 4s - loss: 0.0038 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.3203 - val_accuracy: 0.9256 - val_f1_keras: 0.8600 - lr: 7.0000e-04\n",
      "Epoch 37/500\n",
      "\n",
      "Score 0.9119167352946331. Model not saved.\n",
      "115/115 - 4s - loss: 0.0053 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.2956 - val_accuracy: 0.9278 - val_f1_keras: 0.8626 - lr: 7.0000e-04\n",
      "Epoch 38/500\n",
      "\n",
      "Score 0.9037757860293071. Model not saved.\n",
      "115/115 - 4s - loss: 0.0022 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.4641 - val_accuracy: 0.9190 - val_f1_keras: 0.8494 - lr: 7.0000e-04\n",
      "Epoch 39/500\n",
      "\n",
      "Score 0.9061815693430657. Model not saved.\n",
      "115/115 - 4s - loss: 0.0047 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3568 - val_accuracy: 0.9212 - val_f1_keras: 0.8552 - lr: 7.0000e-04\n",
      "Epoch 40/500\n",
      "\n",
      "Score 0.924394970693893. Model not saved.\n",
      "115/115 - 4s - loss: 0.0044 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.2920 - val_accuracy: 0.9387 - val_f1_keras: 0.8952 - lr: 7.0000e-04\n",
      "Epoch 41/500\n",
      "\n",
      "Score 0.904972043805739. Model not saved.\n",
      "115/115 - 4s - loss: 0.0030 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3302 - val_accuracy: 0.9212 - val_f1_keras: 0.8559 - lr: 7.0000e-04\n",
      "Epoch 42/500\n",
      "\n",
      "Score 0.8989906431886834. Model not saved.\n",
      "115/115 - 4s - loss: 0.0018 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.5201 - val_accuracy: 0.9147 - val_f1_keras: 0.8449 - lr: 7.0000e-04\n",
      "Epoch 43/500\n",
      "\n",
      "Score 0.9152858586522996. Model not saved.\n",
      "115/115 - 4s - loss: 0.0047 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2790 - val_accuracy: 0.9322 - val_f1_keras: 0.8873 - lr: 7.0000e-04\n",
      "Epoch 44/500\n",
      "\n",
      "Score 0.9130803146883375. Model not saved.\n",
      "115/115 - 4s - loss: 0.0041 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.3142 - val_accuracy: 0.9278 - val_f1_keras: 0.8621 - lr: 7.0000e-04\n",
      "Epoch 45/500\n",
      "\n",
      "Score 0.8946607973104406. Model not saved.\n",
      "115/115 - 4s - loss: 0.0052 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.4624 - val_accuracy: 0.9103 - val_f1_keras: 0.8441 - lr: 7.0000e-04\n",
      "Epoch 46/500\n",
      "\n",
      "Score 0.9029629332063149. Model not saved.\n",
      "115/115 - 4s - loss: 0.0040 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.4320 - val_accuracy: 0.9190 - val_f1_keras: 0.8532 - lr: 7.0000e-04\n",
      "Epoch 47/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 0.0029 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.3320 - val_accuracy: 0.9234 - val_f1_keras: 0.8580 - lr: 7.0000e-04\n",
      "Epoch 48/500\n",
      "\n",
      "Score 0.9143914204093102. Model not saved.\n",
      "115/115 - 4s - loss: 0.0031 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3694 - val_accuracy: 0.9300 - val_f1_keras: 0.8650 - lr: 7.0000e-04\n",
      "Epoch 49/500\n",
      "\n",
      "Score 0.9147785547785547. Model not saved.\n",
      "115/115 - 4s - loss: 0.0062 - accuracy: 0.9986 - f1_keras: 0.9984 - val_loss: 0.3060 - val_accuracy: 0.9300 - val_f1_keras: 0.8760 - lr: 7.0000e-04\n",
      "Epoch 50/500\n",
      "\n",
      "Score 0.9009694343065693. Model not saved.\n",
      "115/115 - 4s - loss: 0.0036 - accuracy: 0.9995 - f1_keras: 0.9992 - val_loss: 0.3711 - val_accuracy: 0.9168 - val_f1_keras: 0.8503 - lr: 7.0000e-04\n",
      "Epoch 51/500\n",
      "\n",
      "Score 0.8880450759431651. Model not saved.\n",
      "115/115 - 4s - loss: 0.0029 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.4808 - val_accuracy: 0.9037 - val_f1_keras: 0.8379 - lr: 7.0000e-04\n",
      "Epoch 52/500\n",
      "\n",
      "Score 0.9172551149737462. Model not saved.\n",
      "115/115 - 4s - loss: 0.0028 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.3038 - val_accuracy: 0.9322 - val_f1_keras: 0.8795 - lr: 7.0000e-04\n",
      "Epoch 53/500\n",
      "\n",
      "Score 0.9065783556155198. Model not saved.\n",
      "115/115 - 4s - loss: 0.0026 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.3360 - val_accuracy: 0.9234 - val_f1_keras: 0.8599 - lr: 7.0000e-04\n",
      "Epoch 54/500\n",
      "\n",
      "Score 0.9057861462341946. Model not saved.\n",
      "115/115 - 4s - loss: 0.0025 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.4427 - val_accuracy: 0.9212 - val_f1_keras: 0.8559 - lr: 7.0000e-04\n",
      "Epoch 55/500\n",
      "\n",
      "Score 0.897432444564144. Model not saved.\n",
      "115/115 - 4s - loss: 0.0026 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.6035 - val_accuracy: 0.9125 - val_f1_keras: 0.8446 - lr: 7.0000e-04\n",
      "Epoch 56/500\n",
      "\n",
      "Score 0.9009694343065693. Model not saved.\n",
      "115/115 - 4s - loss: 0.0067 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.4539 - val_accuracy: 0.9168 - val_f1_keras: 0.8517 - lr: 7.0000e-04\n",
      "Epoch 57/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 0.0023 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.3343 - val_accuracy: 0.9278 - val_f1_keras: 0.8734 - lr: 7.0000e-04\n",
      "Epoch 58/500\n",
      "\n",
      "Score 0.9247185869863641. Model not saved.\n",
      "115/115 - 4s - loss: 0.0106 - accuracy: 0.9975 - f1_keras: 0.9974 - val_loss: 0.3078 - val_accuracy: 0.9409 - val_f1_keras: 0.8931 - lr: 7.0000e-04\n",
      "Epoch 59/500\n",
      "\n",
      "Score 0.8972767355407625. Model not saved.\n",
      "115/115 - 4s - loss: 0.0051 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.3494 - val_accuracy: 0.9147 - val_f1_keras: 0.8597 - lr: 7.0000e-04\n",
      "Epoch 60/500\n",
      "\n",
      "Score 0.9164903180170356. Model not saved.\n",
      "115/115 - 4s - loss: 0.0054 - accuracy: 0.9989 - f1_keras: 0.9989 - val_loss: 0.2997 - val_accuracy: 0.9322 - val_f1_keras: 0.8848 - lr: 7.0000e-04\n",
      "Epoch 61/500\n",
      "\n",
      "Score 0.9110202492211839. Model not saved.\n",
      "\n",
      "Epoch 60: early stopping.\n",
      "115/115 - 4s - loss: 0.0030 - accuracy: 0.9992 - f1_keras: 0.9948 - val_loss: 0.4140 - val_accuracy: 0.9256 - val_f1_keras: 0.8593 - lr: 7.0000e-04\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.96      0.95       318\n",
      "         yes       0.91      0.88      0.89       140\n",
      "\n",
      "    accuracy                           0.94       458\n",
      "   macro avg       0.93      0.92      0.92       458\n",
      "weighted avg       0.94      0.94      0.94       458\n",
      " 0.9363525505688748\n",
      "Running Transformer without features for ce loss\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding (T (None, 200, 128)     687360      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 200, 128)     99584       token_and_position_embedding[0][0\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_1 (Transforme (None, 200, 128)     99584       transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           transformer_block_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          16512       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 50, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            258         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 903,298\n",
      "Trainable params: 903,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "\n",
      "Score 0.41857506361323155. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 5s - loss: 0.8553 - accuracy: 0.5566 - f1_keras: 0.4301 - val_loss: 0.6193 - val_accuracy: 0.7199 - val_f1_keras: 0.4168 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "\n",
      "Score 0.6645311978645312. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 4s - loss: 0.6855 - accuracy: 0.6080 - f1_keras: 0.4973 - val_loss: 0.4741 - val_accuracy: 0.7834 - val_f1_keras: 0.6514 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "\n",
      "Score 0.7367859275346238. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 4s - loss: 0.6087 - accuracy: 0.6960 - f1_keras: 0.6334 - val_loss: 0.5208 - val_accuracy: 0.7615 - val_f1_keras: 0.7039 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "\n",
      "Score 0.8096202026602828. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 4s - loss: 0.5170 - accuracy: 0.7589 - f1_keras: 0.7188 - val_loss: 0.3650 - val_accuracy: 0.8446 - val_f1_keras: 0.7685 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "\n",
      "Score 0.8578855033942958. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 4s - loss: 0.3946 - accuracy: 0.8250 - f1_keras: 0.8017 - val_loss: 0.2934 - val_accuracy: 0.8840 - val_f1_keras: 0.8185 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "\n",
      "Score 0.8296738020774255. Model not saved.\n",
      "115/115 - 4s - loss: 0.2556 - accuracy: 0.9043 - f1_keras: 0.8905 - val_loss: 0.3279 - val_accuracy: 0.8490 - val_f1_keras: 0.7944 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "\n",
      "Score 0.9282553711079455. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 4s - loss: 0.1614 - accuracy: 0.9420 - f1_keras: 0.9356 - val_loss: 0.1953 - val_accuracy: 0.9409 - val_f1_keras: 0.8868 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "\n",
      "Score 0.918217609162491. Model not saved.\n",
      "115/115 - 4s - loss: 0.0822 - accuracy: 0.9721 - f1_keras: 0.9695 - val_loss: 0.2588 - val_accuracy: 0.9344 - val_f1_keras: 0.8767 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "\n",
      "Score 0.9257634827810266. Model not saved.\n",
      "115/115 - 4s - loss: 0.0758 - accuracy: 0.9762 - f1_keras: 0.9733 - val_loss: 0.2030 - val_accuracy: 0.9387 - val_f1_keras: 0.8838 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0644 - accuracy: 0.9784 - f1_keras: 0.9761 - val_loss: 0.3164 - val_accuracy: 0.9234 - val_f1_keras: 0.8611 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "\n",
      "Score 0.9441986219728464. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_ce_without_features.h5.\n",
      "115/115 - 5s - loss: 0.0401 - accuracy: 0.9874 - f1_keras: 0.9860 - val_loss: 0.2252 - val_accuracy: 0.9540 - val_f1_keras: 0.9036 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "\n",
      "Score 0.9211525189786058. Model not saved.\n",
      "115/115 - 4s - loss: 0.0257 - accuracy: 0.9915 - f1_keras: 0.9904 - val_loss: 0.3640 - val_accuracy: 0.9344 - val_f1_keras: 0.8794 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "\n",
      "Score 0.9069499117686983. Model not saved.\n",
      "115/115 - 4s - loss: 0.0502 - accuracy: 0.9831 - f1_keras: 0.9806 - val_loss: 0.3179 - val_accuracy: 0.9212 - val_f1_keras: 0.8678 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "\n",
      "Score 0.9265732393108989. Model not saved.\n",
      "115/115 - 4s - loss: 0.0311 - accuracy: 0.9899 - f1_keras: 0.9886 - val_loss: 0.3472 - val_accuracy: 0.9409 - val_f1_keras: 0.8863 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "\n",
      "Score 0.9141784037558685. Model not saved.\n",
      "115/115 - 4s - loss: 0.0273 - accuracy: 0.9899 - f1_keras: 0.9885 - val_loss: 0.4367 - val_accuracy: 0.9278 - val_f1_keras: 0.8692 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "\n",
      "Score 0.9414102564102564. Model not saved.\n",
      "115/115 - 4s - loss: 0.0485 - accuracy: 0.9839 - f1_keras: 0.9811 - val_loss: 0.2964 - val_accuracy: 0.9519 - val_f1_keras: 0.8970 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "\n",
      "Score 0.9017804624225148. Model not saved.\n",
      "115/115 - 4s - loss: 0.0212 - accuracy: 0.9937 - f1_keras: 0.9930 - val_loss: 0.4763 - val_accuracy: 0.9168 - val_f1_keras: 0.8559 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "\n",
      "Score 0.913819409919254. Model not saved.\n",
      "115/115 - 4s - loss: 0.0119 - accuracy: 0.9964 - f1_keras: 0.9962 - val_loss: 0.5159 - val_accuracy: 0.9278 - val_f1_keras: 0.8695 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "\n",
      "Score 0.9113937043795621. Model not saved.\n",
      "115/115 - 4s - loss: 0.0181 - accuracy: 0.9945 - f1_keras: 0.9937 - val_loss: 0.3367 - val_accuracy: 0.9256 - val_f1_keras: 0.8714 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "\n",
      "Score 0.9204608744082428. Model not saved.\n",
      "115/115 - 4s - loss: 0.0129 - accuracy: 0.9964 - f1_keras: 0.9960 - val_loss: 0.4214 - val_accuracy: 0.9344 - val_f1_keras: 0.8802 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "\n",
      "Score 0.9045578980046849. Model not saved.\n",
      "115/115 - 4s - loss: 0.0062 - accuracy: 0.9978 - f1_keras: 0.9976 - val_loss: 0.4966 - val_accuracy: 0.9190 - val_f1_keras: 0.8481 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "\n",
      "Score 0.9297953299300435. Model not saved.\n",
      "115/115 - 4s - loss: 0.0195 - accuracy: 0.9945 - f1_keras: 0.9938 - val_loss: 0.3624 - val_accuracy: 0.9431 - val_f1_keras: 0.8943 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "\n",
      "Score 0.9147785547785547. Model not saved.\n",
      "115/115 - 4s - loss: 0.0160 - accuracy: 0.9945 - f1_keras: 0.9938 - val_loss: 0.3621 - val_accuracy: 0.9300 - val_f1_keras: 0.8711 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "\n",
      "Score 0.9037757860293071. Model not saved.\n",
      "115/115 - 4s - loss: 0.0290 - accuracy: 0.9913 - f1_keras: 0.9901 - val_loss: 0.4725 - val_accuracy: 0.9190 - val_f1_keras: 0.8498 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "\n",
      "Score 0.8993988654643975. Model not saved.\n",
      "115/115 - 4s - loss: 0.0062 - accuracy: 0.9978 - f1_keras: 0.9976 - val_loss: 0.6504 - val_accuracy: 0.9147 - val_f1_keras: 0.8430 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "\n",
      "Score 0.9045578980046849. Model not saved.\n",
      "115/115 - 4s - loss: 0.0062 - accuracy: 0.9986 - f1_keras: 0.9984 - val_loss: 0.5663 - val_accuracy: 0.9190 - val_f1_keras: 0.8484 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "\n",
      "Score 0.9057861462341946. Model not saved.\n",
      "115/115 - 4s - loss: 0.0036 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.5001 - val_accuracy: 0.9212 - val_f1_keras: 0.8500 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "\n",
      "Score 0.9033732777882544. Model not saved.\n",
      "115/115 - 4s - loss: 0.0021 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.5185 - val_accuracy: 0.9190 - val_f1_keras: 0.8462 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "\n",
      "Score 0.9037757860293071. Model not saved.\n",
      "115/115 - 4s - loss: 0.0022 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.5410 - val_accuracy: 0.9190 - val_f1_keras: 0.8459 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "\n",
      "Score 0.9033732777882544. Model not saved.\n",
      "115/115 - 4s - loss: 0.0021 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.5599 - val_accuracy: 0.9190 - val_f1_keras: 0.8462 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "\n",
      "Score 0.8997992950026142. Model not saved.\n",
      "115/115 - 4s - loss: 0.0022 - accuracy: 0.9986 - f1_keras: 0.9983 - val_loss: 0.6868 - val_accuracy: 0.9147 - val_f1_keras: 0.8436 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "\n",
      "Score 0.9017804624225148. Model not saved.\n",
      "115/115 - 4s - loss: 0.0021 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.5955 - val_accuracy: 0.9168 - val_f1_keras: 0.8438 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "\n",
      "Score 0.9053830227743271. Model not saved.\n",
      "115/115 - 4s - loss: 0.0015 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.5381 - val_accuracy: 0.9212 - val_f1_keras: 0.8480 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "\n",
      "Score 0.9089770948925879. Model not saved.\n",
      "115/115 - 4s - loss: 0.0011 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6043 - val_accuracy: 0.9234 - val_f1_keras: 0.8505 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "\n",
      "Score 0.9017804624225148. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.6674 - val_accuracy: 0.9168 - val_f1_keras: 0.8438 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "\n",
      "Score 0.8996927129060579. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9995 - f1_keras: 0.9995 - val_loss: 0.5885 - val_accuracy: 0.9168 - val_f1_keras: 0.8439 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0019 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.6038 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "\n",
      "Score 0.9069499117686983. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.6898 - val_accuracy: 0.9212 - val_f1_keras: 0.8511 - lr: 7.0000e-04\n",
      "Epoch 39/500\n",
      "\n",
      "Score 0.9065694425514514. Model not saved.\n",
      "115/115 - 4s - loss: 0.0011 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6760 - val_accuracy: 0.9212 - val_f1_keras: 0.8484 - lr: 7.0000e-04\n",
      "Epoch 40/500\n",
      "\n",
      "Score 0.9113937043795621. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6576 - val_accuracy: 0.9256 - val_f1_keras: 0.8525 - lr: 7.0000e-04\n",
      "Epoch 41/500\n",
      "\n",
      "Score 0.8996927129060579. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6077 - val_accuracy: 0.9168 - val_f1_keras: 0.8439 - lr: 7.0000e-04\n",
      "Epoch 42/500\n",
      "\n",
      "Score 0.9089770948925879. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.6984 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 7.0000e-04\n",
      "Epoch 43/500\n",
      "\n",
      "Score 0.9025445952566209. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6000 - val_accuracy: 0.9190 - val_f1_keras: 0.8462 - lr: 7.0000e-04\n",
      "Epoch 44/500\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6297 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 7.0000e-04\n",
      "Epoch 45/500\n",
      "\n",
      "Score 0.9041706102046483. Model not saved.\n",
      "115/115 - 4s - loss: 0.0011 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6927 - val_accuracy: 0.9190 - val_f1_keras: 0.8463 - lr: 7.0000e-04\n",
      "Epoch 46/500\n",
      "\n",
      "Score 0.9113937043795621. Model not saved.\n",
      "115/115 - 4s - loss: 0.0016 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.6605 - val_accuracy: 0.9256 - val_f1_keras: 0.8525 - lr: 7.0000e-04\n",
      "Epoch 47/500\n",
      "\n",
      "Score 0.9089770948925879. Model not saved.\n",
      "115/115 - 4s - loss: 0.0010 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.6833 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 7.0000e-04\n",
      "Epoch 48/500\n",
      "\n",
      "Score 0.9089770948925879. Model not saved.\n",
      "115/115 - 4s - loss: 0.0017 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.6822 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 7.0000e-04\n",
      "Epoch 49/500\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9993 - val_loss: 0.6537 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 7.0000e-04\n",
      "Epoch 50/500\n",
      "\n",
      "Score 0.9065694425514514. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9989 - f1_keras: 0.9984 - val_loss: 0.7007 - val_accuracy: 0.9212 - val_f1_keras: 0.8483 - lr: 7.0000e-04\n",
      "Epoch 51/500\n",
      "\n",
      "Score 0.9069499117686983. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.7312 - val_accuracy: 0.9212 - val_f1_keras: 0.8511 - lr: 7.0000e-04\n",
      "Epoch 52/500\n",
      "\n",
      "Score 0.9053830227743271. Model not saved.\n",
      "115/115 - 4s - loss: 0.0014 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6528 - val_accuracy: 0.9212 - val_f1_keras: 0.8480 - lr: 7.0000e-04\n",
      "Epoch 53/500\n",
      "\n",
      "Score 0.9025445952566209. Model not saved.\n",
      "115/115 - 4s - loss: 0.0011 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6668 - val_accuracy: 0.9190 - val_f1_keras: 0.8462 - lr: 7.0000e-04\n",
      "Epoch 54/500\n",
      "\n",
      "Score 0.9069499117686983. Model not saved.\n",
      "115/115 - 4s - loss: 0.0011 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.7570 - val_accuracy: 0.9212 - val_f1_keras: 0.8511 - lr: 7.0000e-04\n",
      "Epoch 55/500\n",
      "\n",
      "Score 0.9093505772206134. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.7372 - val_accuracy: 0.9234 - val_f1_keras: 0.8531 - lr: 7.0000e-04\n",
      "Epoch 56/500\n",
      "\n",
      "Score 0.9069499117686983. Model not saved.\n",
      "115/115 - 4s - loss: 0.0018 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.7165 - val_accuracy: 0.9212 - val_f1_keras: 0.8511 - lr: 7.0000e-04\n",
      "Epoch 57/500\n",
      "\n",
      "Score 0.9069977032880774. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.5929 - val_accuracy: 0.9234 - val_f1_keras: 0.8636 - lr: 7.0000e-04\n",
      "Epoch 58/500\n",
      "\n",
      "Score 0.9053830227743271. Model not saved.\n",
      "115/115 - 4s - loss: 0.0017 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.6507 - val_accuracy: 0.9212 - val_f1_keras: 0.8480 - lr: 7.0000e-04\n",
      "Epoch 59/500\n",
      "\n",
      "Score 0.9110202492211839. Model not saved.\n",
      "115/115 - 4s - loss: 0.0019 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.6388 - val_accuracy: 0.9256 - val_f1_keras: 0.8525 - lr: 7.0000e-04\n",
      "Epoch 60/500\n",
      "\n",
      "Score 0.9082081800600277. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.6709 - val_accuracy: 0.9234 - val_f1_keras: 0.8504 - lr: 7.0000e-04\n",
      "Epoch 61/500\n",
      "\n",
      "Score 0.9069499117686983. Model not saved.\n",
      "\n",
      "Epoch 60: early stopping.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9951 - val_loss: 0.8093 - val_accuracy: 0.9212 - val_f1_keras: 0.8511 - lr: 7.0000e-04\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.96      0.96      0.96       318\n",
      "         yes       0.90      0.91      0.90       140\n",
      "\n",
      "    accuracy                           0.94       458\n",
      "   macro avg       0.93      0.93      0.93       458\n",
      "weighted avg       0.94      0.94      0.94       458\n",
      " 0.9411065127993595\n",
      "Running Transformer with features for focal loss\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding (T (None, 200, 128)     687360      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 200, 128)     99584       token_and_position_embedding[0][0\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_1 (Transforme (None, 200, 128)     99584       transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           transformer_block_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 18201)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 18329)        0           dropout_4[0][0]                  \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          2346240     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 50, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            258         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,233,026\n",
      "Trainable params: 3,233,026\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "\n",
      "Score 0.8333362277060938. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_with_features.h5.\n",
      "115/115 - 5s - loss: 0.1873 - accuracy: 0.6091 - f1_keras: 0.4958 - val_loss: 0.0986 - val_accuracy: 0.8621 - val_f1_keras: 0.7866 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "\n",
      "Score 0.7668250055694092. Model not saved.\n",
      "115/115 - 4s - loss: 0.1028 - accuracy: 0.8305 - f1_keras: 0.7927 - val_loss: 0.0784 - val_accuracy: 0.8446 - val_f1_keras: 0.7200 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "\n",
      "Score 0.9250924928581464. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_with_features.h5.\n",
      "115/115 - 5s - loss: 0.0586 - accuracy: 0.9259 - f1_keras: 0.9157 - val_loss: 0.0510 - val_accuracy: 0.9387 - val_f1_keras: 0.8941 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "\n",
      "Score 0.9037757860293071. Model not saved.\n",
      "115/115 - 4s - loss: 0.0265 - accuracy: 0.9765 - f1_keras: 0.9739 - val_loss: 0.0596 - val_accuracy: 0.9190 - val_f1_keras: 0.8631 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "\n",
      "Score 0.9172551149737462. Model not saved.\n",
      "115/115 - 4s - loss: 0.0122 - accuracy: 0.9910 - f1_keras: 0.9902 - val_loss: 0.0552 - val_accuracy: 0.9322 - val_f1_keras: 0.8806 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "\n",
      "Score 0.9106395215090868. Model not saved.\n",
      "115/115 - 4s - loss: 0.0065 - accuracy: 0.9973 - f1_keras: 0.9970 - val_loss: 0.0555 - val_accuracy: 0.9256 - val_f1_keras: 0.8719 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "\n",
      "Score 0.9155307056051013. Model not saved.\n",
      "115/115 - 4s - loss: 0.0051 - accuracy: 0.9981 - f1_keras: 0.9976 - val_loss: 0.0611 - val_accuracy: 0.9300 - val_f1_keras: 0.8754 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "\n",
      "Score 0.9179908422045859. Model not saved.\n",
      "115/115 - 4s - loss: 0.0031 - accuracy: 0.9984 - f1_keras: 0.9982 - val_loss: 0.0598 - val_accuracy: 0.9322 - val_f1_keras: 0.8766 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "\n",
      "Score 0.9126999287984301. Model not saved.\n",
      "115/115 - 4s - loss: 0.0038 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.0781 - val_accuracy: 0.9278 - val_f1_keras: 0.8746 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "\n",
      "Score 0.915158266035459. Model not saved.\n",
      "115/115 - 4s - loss: 0.0044 - accuracy: 0.9984 - f1_keras: 0.9980 - val_loss: 0.0792 - val_accuracy: 0.9300 - val_f1_keras: 0.8776 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "\n",
      "Score 0.9033732777882544. Model not saved.\n",
      "115/115 - 4s - loss: 0.0043 - accuracy: 0.9962 - f1_keras: 0.9954 - val_loss: 0.0986 - val_accuracy: 0.9190 - val_f1_keras: 0.8659 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "\n",
      "Score 0.9005520432472054. Model not saved.\n",
      "115/115 - 4s - loss: 0.0058 - accuracy: 0.9945 - f1_keras: 0.9937 - val_loss: 0.1025 - val_accuracy: 0.9168 - val_f1_keras: 0.8591 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "\n",
      "Score 0.9279318743319724. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_with_features.h5.\n",
      "115/115 - 5s - loss: 0.0035 - accuracy: 0.9984 - f1_keras: 0.9976 - val_loss: 0.1134 - val_accuracy: 0.9409 - val_f1_keras: 0.8880 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "\n",
      "Score 0.9005520432472054. Model not saved.\n",
      "115/115 - 4s - loss: 0.0039 - accuracy: 0.9975 - f1_keras: 0.9973 - val_loss: 0.0954 - val_accuracy: 0.9168 - val_f1_keras: 0.8602 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "\n",
      "Score 0.9130803146883375. Model not saved.\n",
      "115/115 - 4s - loss: 0.0036 - accuracy: 0.9984 - f1_keras: 0.9978 - val_loss: 0.0823 - val_accuracy: 0.9278 - val_f1_keras: 0.8750 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "\n",
      "Score 0.9201048951048951. Model not saved.\n",
      "115/115 - 4s - loss: 0.0055 - accuracy: 0.9954 - f1_keras: 0.9947 - val_loss: 0.0879 - val_accuracy: 0.9344 - val_f1_keras: 0.8829 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "\n",
      "Score 0.9176265371980115. Model not saved.\n",
      "115/115 - 4s - loss: 0.0022 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.0941 - val_accuracy: 0.9322 - val_f1_keras: 0.8800 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "\n",
      "Score 0.9408727358268643. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_with_features.h5.\n",
      "115/115 - 5s - loss: 0.0023 - accuracy: 0.9984 - f1_keras: 0.9981 - val_loss: 0.0856 - val_accuracy: 0.9519 - val_f1_keras: 0.9098 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "\n",
      "Score 0.9345740873299928. Model not saved.\n",
      "115/115 - 4s - loss: 0.0020 - accuracy: 0.9992 - f1_keras: 0.9989 - val_loss: 0.0815 - val_accuracy: 0.9475 - val_f1_keras: 0.8955 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "\n",
      "Score 0.9285726690168974. Model not saved.\n",
      "115/115 - 4s - loss: 0.0031 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1030 - val_accuracy: 0.9409 - val_f1_keras: 0.8878 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "\n",
      "Score 0.9053830227743271. Model not saved.\n",
      "115/115 - 4s - loss: 0.0020 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1044 - val_accuracy: 0.9212 - val_f1_keras: 0.8678 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "\n",
      "Score 0.9183481744041959. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1669 - val_accuracy: 0.9322 - val_f1_keras: 0.8783 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "\n",
      "Score 0.9009694343065693. Model not saved.\n",
      "115/115 - 4s - loss: 0.0031 - accuracy: 0.9984 - f1_keras: 0.9983 - val_loss: 0.1152 - val_accuracy: 0.9168 - val_f1_keras: 0.8595 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "\n",
      "Score 0.9033732777882544. Model not saved.\n",
      "115/115 - 4s - loss: 0.0048 - accuracy: 0.9973 - f1_keras: 0.9967 - val_loss: 0.0953 - val_accuracy: 0.9190 - val_f1_keras: 0.8638 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "\n",
      "Score 0.9025608223359367. Model not saved.\n",
      "115/115 - 4s - loss: 0.0020 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1224 - val_accuracy: 0.9168 - val_f1_keras: 0.8596 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "\n",
      "Score 0.9279318743319724. Model not saved.\n",
      "115/115 - 4s - loss: 0.0025 - accuracy: 0.9981 - f1_keras: 0.9979 - val_loss: 0.0946 - val_accuracy: 0.9409 - val_f1_keras: 0.8987 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "\n",
      "Score 0.9269228719151432. Model not saved.\n",
      "115/115 - 4s - loss: 0.0051 - accuracy: 0.9959 - f1_keras: 0.9955 - val_loss: 0.1113 - val_accuracy: 0.9409 - val_f1_keras: 0.8969 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "\n",
      "Score 0.9013788560265322. Model not saved.\n",
      "115/115 - 4s - loss: 0.0029 - accuracy: 0.9981 - f1_keras: 0.9979 - val_loss: 0.1000 - val_accuracy: 0.9168 - val_f1_keras: 0.8598 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "\n",
      "Score 0.8880450759431651. Model not saved.\n",
      "115/115 - 4s - loss: 0.0025 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1427 - val_accuracy: 0.9037 - val_f1_keras: 0.8441 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "\n",
      "Score 0.9134534269137403. Model not saved.\n",
      "115/115 - 4s - loss: 0.0022 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.0973 - val_accuracy: 0.9278 - val_f1_keras: 0.8713 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "\n",
      "Score 0.9288838938359125. Model not saved.\n",
      "115/115 - 4s - loss: 0.0021 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1434 - val_accuracy: 0.9409 - val_f1_keras: 0.8833 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "\n",
      "Score 0.8888815207780725. Model not saved.\n",
      "115/115 - 4s - loss: 0.0045 - accuracy: 0.9984 - f1_keras: 0.9982 - val_loss: 0.1883 - val_accuracy: 0.9037 - val_f1_keras: 0.8444 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Score 0.9232817556107416. Model not saved.\n",
      "115/115 - 4s - loss: 0.0032 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.0766 - val_accuracy: 0.9365 - val_f1_keras: 0.8829 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "\n",
      "Score 0.9013788560265322. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1059 - val_accuracy: 0.9168 - val_f1_keras: 0.8604 - lr: 7.0000e-04\n",
      "Epoch 35/500\n",
      "\n",
      "Score 0.9380412036900587. Model not saved.\n",
      "115/115 - 4s - loss: 0.0029 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.0864 - val_accuracy: 0.9497 - val_f1_keras: 0.9064 - lr: 7.0000e-04\n",
      "Epoch 36/500\n",
      "\n",
      "Score 0.9176265371980115. Model not saved.\n",
      "115/115 - 4s - loss: 5.1756e-04 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.0978 - val_accuracy: 0.9322 - val_f1_keras: 0.8814 - lr: 7.0000e-04\n",
      "Epoch 37/500\n",
      "\n",
      "Score 0.9304430290825645. Model not saved.\n",
      "115/115 - 4s - loss: 0.0028 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.1021 - val_accuracy: 0.9431 - val_f1_keras: 0.9001 - lr: 7.0000e-04\n",
      "Epoch 38/500\n",
      "\n",
      "Score 0.9113937043795621. Model not saved.\n",
      "115/115 - 4s - loss: 8.5714e-04 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.1462 - val_accuracy: 0.9256 - val_f1_keras: 0.8671 - lr: 7.0000e-04\n",
      "Epoch 39/500\n",
      "\n",
      "Score 0.9089770948925879. Model not saved.\n",
      "115/115 - 4s - loss: 0.0016 - accuracy: 0.9989 - f1_keras: 0.9989 - val_loss: 0.1046 - val_accuracy: 0.9234 - val_f1_keras: 0.8688 - lr: 7.0000e-04\n",
      "Epoch 40/500\n",
      "\n",
      "Score 0.9400262467191601. Model not saved.\n",
      "115/115 - 4s - loss: 0.0023 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.0997 - val_accuracy: 0.9519 - val_f1_keras: 0.9092 - lr: 7.0000e-04\n",
      "Epoch 41/500\n",
      "\n",
      "Score 0.9232817556107416. Model not saved.\n",
      "115/115 - 4s - loss: 0.0010 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.0990 - val_accuracy: 0.9365 - val_f1_keras: 0.8860 - lr: 7.0000e-04\n",
      "Epoch 42/500\n",
      "\n",
      "Score 0.9025608223359367. Model not saved.\n",
      "115/115 - 4s - loss: 9.4218e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1511 - val_accuracy: 0.9168 - val_f1_keras: 0.8585 - lr: 7.0000e-04\n",
      "Epoch 43/500\n",
      "\n",
      "Score 0.9320122586212027. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.0932 - val_accuracy: 0.9453 - val_f1_keras: 0.9034 - lr: 7.0000e-04\n",
      "Epoch 44/500\n",
      "\n",
      "Score 0.9211525189786058. Model not saved.\n",
      "115/115 - 4s - loss: 5.9935e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1330 - val_accuracy: 0.9344 - val_f1_keras: 0.8822 - lr: 7.0000e-04\n",
      "Epoch 45/500\n",
      "\n",
      "Score 0.9025608223359367. Model not saved.\n",
      "115/115 - 4s - loss: 0.0018 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.1380 - val_accuracy: 0.9168 - val_f1_keras: 0.8633 - lr: 7.0000e-04\n",
      "Epoch 46/500\n",
      "\n",
      "Score 0.9065694425514514. Model not saved.\n",
      "115/115 - 4s - loss: 0.0015 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1519 - val_accuracy: 0.9212 - val_f1_keras: 0.8668 - lr: 7.0000e-04\n",
      "Epoch 47/500\n",
      "\n",
      "Score 0.9113937043795621. Model not saved.\n",
      "115/115 - 4s - loss: 1.8567e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1540 - val_accuracy: 0.9256 - val_f1_keras: 0.8726 - lr: 7.0000e-04\n",
      "Epoch 48/500\n",
      "\n",
      "Score 0.9013788560265322. Model not saved.\n",
      "115/115 - 4s - loss: 0.0011 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1350 - val_accuracy: 0.9168 - val_f1_keras: 0.8631 - lr: 7.0000e-04\n",
      "Epoch 49/500\n",
      "\n",
      "Score 0.8997992950026142. Model not saved.\n",
      "115/115 - 4s - loss: 0.0028 - accuracy: 0.9984 - f1_keras: 0.9982 - val_loss: 0.1274 - val_accuracy: 0.9147 - val_f1_keras: 0.8607 - lr: 7.0000e-04\n",
      "Epoch 50/500\n",
      "\n",
      "Score 0.9117600290763709. Model not saved.\n",
      "115/115 - 4s - loss: 9.6381e-04 - accuracy: 0.9986 - f1_keras: 0.9982 - val_loss: 0.1254 - val_accuracy: 0.9256 - val_f1_keras: 0.8704 - lr: 7.0000e-04\n",
      "Epoch 51/500\n",
      "\n",
      "Score 0.8884673411430599. Model not saved.\n",
      "115/115 - 4s - loss: 6.8033e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.2201 - val_accuracy: 0.9037 - val_f1_keras: 0.8445 - lr: 7.0000e-04\n",
      "Epoch 52/500\n",
      "\n",
      "Score 0.9405960484023446. Model not saved.\n",
      "115/115 - 4s - loss: 7.4552e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.0825 - val_accuracy: 0.9519 - val_f1_keras: 0.9110 - lr: 7.0000e-04\n",
      "Epoch 53/500\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0017 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1119 - val_accuracy: 0.9234 - val_f1_keras: 0.8721 - lr: 7.0000e-04\n",
      "Epoch 54/500\n",
      "\n",
      "Score 0.9013788560265322. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1441 - val_accuracy: 0.9168 - val_f1_keras: 0.8596 - lr: 7.0000e-04\n",
      "Epoch 55/500\n",
      "\n",
      "Score 0.89120553633218. Model not saved.\n",
      "115/115 - 4s - loss: 0.0013 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2057 - val_accuracy: 0.9059 - val_f1_keras: 0.8460 - lr: 7.0000e-04\n",
      "Epoch 56/500\n",
      "\n",
      "Score 0.8833976748313808. Model not saved.\n",
      "115/115 - 4s - loss: 0.0018 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.1532 - val_accuracy: 0.8993 - val_f1_keras: 0.8407 - lr: 7.0000e-04\n",
      "Epoch 57/500\n",
      "\n",
      "Score 0.9197419566337284. Model not saved.\n",
      "115/115 - 4s - loss: 0.0019 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.0988 - val_accuracy: 0.9344 - val_f1_keras: 0.8915 - lr: 7.0000e-04\n",
      "Epoch 58/500\n",
      "\n",
      "Score 0.930294987950337. Model not saved.\n",
      "115/115 - 4s - loss: 0.0018 - accuracy: 0.9978 - f1_keras: 0.9977 - val_loss: 0.0987 - val_accuracy: 0.9453 - val_f1_keras: 0.8947 - lr: 7.0000e-04\n",
      "Epoch 59/500\n",
      "\n",
      "Score 0.9126999287984301. Model not saved.\n",
      "115/115 - 4s - loss: 0.0025 - accuracy: 0.9986 - f1_keras: 0.9984 - val_loss: 0.1149 - val_accuracy: 0.9278 - val_f1_keras: 0.8835 - lr: 7.0000e-04\n",
      "Epoch 60/500\n",
      "\n",
      "Score 0.9232817556107416. Model not saved.\n",
      "115/115 - 4s - loss: 0.0026 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.1042 - val_accuracy: 0.9365 - val_f1_keras: 0.8929 - lr: 7.0000e-04\n",
      "Epoch 61/500\n",
      "\n",
      "Score 0.9357935653069827. Model not saved.\n",
      "115/115 - 4s - loss: 0.0018 - accuracy: 0.9989 - f1_keras: 0.9945 - val_loss: 0.0948 - val_accuracy: 0.9475 - val_f1_keras: 0.9056 - lr: 7.0000e-04\n",
      "Epoch 62/500\n",
      "\n",
      "Score 0.902174402884182. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1603 - val_accuracy: 0.9168 - val_f1_keras: 0.8473 - lr: 7.0000e-04\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\n",
      "Score 0.9110202492211839. Model not saved.\n",
      "115/115 - 4s - loss: 4.5153e-04 - accuracy: 0.9997 - f1_keras: 0.9997 - val_loss: 0.1169 - val_accuracy: 0.9256 - val_f1_keras: 0.8720 - lr: 7.0000e-04\n",
      "Epoch 64/500\n",
      "\n",
      "Score 0.9332702540110855. Model not saved.\n",
      "115/115 - 4s - loss: 0.0022 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1126 - val_accuracy: 0.9453 - val_f1_keras: 0.8931 - lr: 4.9000e-04\n",
      "Epoch 65/500\n",
      "\n",
      "Score 0.8927230046948357. Model not saved.\n",
      "115/115 - 4s - loss: 3.9675e-04 - accuracy: 0.9997 - f1_keras: 0.9997 - val_loss: 0.1597 - val_accuracy: 0.9081 - val_f1_keras: 0.8386 - lr: 4.9000e-04\n",
      "Epoch 66/500\n",
      "\n",
      "Score 0.9211525189786058. Model not saved.\n",
      "115/115 - 4s - loss: 0.0015 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1154 - val_accuracy: 0.9344 - val_f1_keras: 0.8817 - lr: 4.9000e-04\n",
      "Epoch 67/500\n",
      "\n",
      "Score 0.8950737233512345. Model not saved.\n",
      "115/115 - 4s - loss: 9.3105e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1549 - val_accuracy: 0.9103 - val_f1_keras: 0.8406 - lr: 4.9000e-04\n",
      "Epoch 68/500\n",
      "\n",
      "Score 0.8993988654643975. Model not saved.\n",
      "\n",
      "Epoch 67: early stopping.\n",
      "115/115 - 4s - loss: 0.0017 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1304 - val_accuracy: 0.9147 - val_f1_keras: 0.8496 - lr: 4.9000e-04\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.94      0.94       318\n",
      "         yes       0.87      0.89      0.88       140\n",
      "\n",
      "    accuracy                           0.92       458\n",
      "   macro avg       0.91      0.91      0.91       458\n",
      "weighted avg       0.92      0.92      0.92       458\n",
      " 0.9238047466787457\n",
      "Running Transformer without features for focal loss\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding (T (None, 200, 128)     687360      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 200, 128)     99584       token_and_position_embedding[0][0\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_1 (Transforme (None, 200, 128)     99584       transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 128)          0           transformer_block_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          16512       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 50, 20)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            258         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 903,298\n",
      "Trainable params: 903,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "\n",
      "Score 0.2188034188034188. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 5s - loss: 0.3758 - accuracy: 0.5426 - f1_keras: 0.4689 - val_loss: 0.1965 - val_accuracy: 0.2801 - val_f1_keras: 0.2016 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "\n",
      "Score 0.6638293400588483. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.1956 - accuracy: 0.5984 - f1_keras: 0.5330 - val_loss: 0.1141 - val_accuracy: 0.7856 - val_f1_keras: 0.6430 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "\n",
      "Score 0.8064925899788284. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.1760 - accuracy: 0.6591 - f1_keras: 0.6031 - val_loss: 0.1015 - val_accuracy: 0.8425 - val_f1_keras: 0.7766 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "\n",
      "Score 0.7352047952047952. Model not saved.\n",
      "115/115 - 4s - loss: 0.1358 - accuracy: 0.7668 - f1_keras: 0.7349 - val_loss: 0.1435 - val_accuracy: 0.7462 - val_f1_keras: 0.6841 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "\n",
      "Score 0.8636567052856645. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0965 - accuracy: 0.8532 - f1_keras: 0.8345 - val_loss: 0.0643 - val_accuracy: 0.8950 - val_f1_keras: 0.8347 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "\n",
      "Score 0.8811026650377811. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0601 - accuracy: 0.9117 - f1_keras: 0.9029 - val_loss: 0.0827 - val_accuracy: 0.8993 - val_f1_keras: 0.8450 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "\n",
      "Score 0.890083837273227. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0334 - accuracy: 0.9533 - f1_keras: 0.9470 - val_loss: 0.0815 - val_accuracy: 0.9081 - val_f1_keras: 0.8523 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "\n",
      "Score 0.8954787604942402. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0222 - accuracy: 0.9738 - f1_keras: 0.9716 - val_loss: 0.1614 - val_accuracy: 0.9103 - val_f1_keras: 0.8421 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "\n",
      "Score 0.8918769716088328. Model not saved.\n",
      "115/115 - 4s - loss: 0.0181 - accuracy: 0.9792 - f1_keras: 0.9772 - val_loss: 0.1216 - val_accuracy: 0.9081 - val_f1_keras: 0.8394 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "\n",
      "Score 0.8843838019260933. Model not saved.\n",
      "115/115 - 4s - loss: 0.0115 - accuracy: 0.9866 - f1_keras: 0.9851 - val_loss: 0.1382 - val_accuracy: 0.9015 - val_f1_keras: 0.8336 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "\n",
      "Score 0.8918769716088328. Model not saved.\n",
      "115/115 - 4s - loss: 0.0124 - accuracy: 0.9880 - f1_keras: 0.9862 - val_loss: 0.1386 - val_accuracy: 0.9081 - val_f1_keras: 0.8550 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "\n",
      "Score 0.891441563730148. Model not saved.\n",
      "115/115 - 4s - loss: 0.0178 - accuracy: 0.9787 - f1_keras: 0.9763 - val_loss: 0.1323 - val_accuracy: 0.9081 - val_f1_keras: 0.8505 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "\n",
      "Score 0.8719064813440832. Model not saved.\n",
      "115/115 - 4s - loss: 0.0069 - accuracy: 0.9934 - f1_keras: 0.9926 - val_loss: 0.2614 - val_accuracy: 0.8884 - val_f1_keras: 0.8207 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "\n",
      "Score 0.895757299270073. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0054 - accuracy: 0.9940 - f1_keras: 0.9933 - val_loss: 0.1450 - val_accuracy: 0.9125 - val_f1_keras: 0.8563 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "\n",
      "Score 0.8931339361275666. Model not saved.\n",
      "115/115 - 4s - loss: 0.0085 - accuracy: 0.9874 - f1_keras: 0.9858 - val_loss: 0.2124 - val_accuracy: 0.9081 - val_f1_keras: 0.8527 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "\n",
      "Score 0.909376070379117. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0088 - accuracy: 0.9902 - f1_keras: 0.9892 - val_loss: 0.0899 - val_accuracy: 0.9278 - val_f1_keras: 0.8763 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "\n",
      "Score 0.8981502117227547. Model not saved.\n",
      "115/115 - 4s - loss: 0.0069 - accuracy: 0.9913 - f1_keras: 0.9899 - val_loss: 0.1248 - val_accuracy: 0.9147 - val_f1_keras: 0.8559 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "\n",
      "Score 0.8993988654643975. Model not saved.\n",
      "115/115 - 4s - loss: 0.0024 - accuracy: 0.9975 - f1_keras: 0.9972 - val_loss: 0.2164 - val_accuracy: 0.9147 - val_f1_keras: 0.8580 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "\n",
      "Score 0.9179908422045859. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0017 - accuracy: 0.9986 - f1_keras: 0.9982 - val_loss: 0.1448 - val_accuracy: 0.9322 - val_f1_keras: 0.8721 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "\n",
      "Score 0.9085963438537541. Model not saved.\n",
      "115/115 - 4s - loss: 0.0024 - accuracy: 0.9973 - f1_keras: 0.9970 - val_loss: 0.1653 - val_accuracy: 0.9234 - val_f1_keras: 0.8640 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "\n",
      "Score 0.9021181019861186. Model not saved.\n",
      "115/115 - 4s - loss: 0.0056 - accuracy: 0.9945 - f1_keras: 0.9940 - val_loss: 0.1001 - val_accuracy: 0.9190 - val_f1_keras: 0.8685 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "\n",
      "Score 0.8741906355988196. Model not saved.\n",
      "115/115 - 4s - loss: 0.0086 - accuracy: 0.9923 - f1_keras: 0.9910 - val_loss: 0.1702 - val_accuracy: 0.8906 - val_f1_keras: 0.8438 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "\n",
      "Score 0.8764817174408277. Model not saved.\n",
      "115/115 - 4s - loss: 0.0085 - accuracy: 0.9893 - f1_keras: 0.9888 - val_loss: 0.1790 - val_accuracy: 0.8928 - val_f1_keras: 0.8399 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "\n",
      "Score 0.9147785547785547. Model not saved.\n",
      "115/115 - 4s - loss: 0.0094 - accuracy: 0.9891 - f1_keras: 0.9880 - val_loss: 0.1284 - val_accuracy: 0.9300 - val_f1_keras: 0.8774 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "\n",
      "Score 0.915158266035459. Model not saved.\n",
      "115/115 - 4s - loss: 0.0041 - accuracy: 0.9954 - f1_keras: 0.9947 - val_loss: 0.1394 - val_accuracy: 0.9300 - val_f1_keras: 0.8770 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "\n",
      "Score 0.9009694343065693. Model not saved.\n",
      "115/115 - 4s - loss: 0.0029 - accuracy: 0.9964 - f1_keras: 0.9953 - val_loss: 0.1339 - val_accuracy: 0.9168 - val_f1_keras: 0.8640 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "\n",
      "Score 0.8946607973104406. Model not saved.\n",
      "115/115 - 4s - loss: 0.0084 - accuracy: 0.9915 - f1_keras: 0.9904 - val_loss: 0.1213 - val_accuracy: 0.9103 - val_f1_keras: 0.8560 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "\n",
      "Score 0.9113937043795621. Model not saved.\n",
      "115/115 - 4s - loss: 0.0044 - accuracy: 0.9967 - f1_keras: 0.9964 - val_loss: 0.1829 - val_accuracy: 0.9256 - val_f1_keras: 0.8727 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "\n",
      "Score 0.9158960202438463. Model not saved.\n",
      "115/115 - 4s - loss: 0.0044 - accuracy: 0.9954 - f1_keras: 0.9943 - val_loss: 0.1262 - val_accuracy: 0.9300 - val_f1_keras: 0.8779 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "\n",
      "Score 0.8961882695016128. Model not saved.\n",
      "115/115 - 4s - loss: 0.0066 - accuracy: 0.9923 - f1_keras: 0.9915 - val_loss: 0.1282 - val_accuracy: 0.9125 - val_f1_keras: 0.8531 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "\n",
      "Score 0.9130803146883375. Model not saved.\n",
      "115/115 - 4s - loss: 0.0026 - accuracy: 0.9970 - f1_keras: 0.9965 - val_loss: 0.1280 - val_accuracy: 0.9278 - val_f1_keras: 0.8723 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "\n",
      "Score 0.904972043805739. Model not saved.\n",
      "115/115 - 4s - loss: 0.0038 - accuracy: 0.9954 - f1_keras: 0.9948 - val_loss: 0.1685 - val_accuracy: 0.9212 - val_f1_keras: 0.8672 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "\n",
      "Score 0.9222392377063128. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 4s - loss: 0.0041 - accuracy: 0.9943 - f1_keras: 0.9938 - val_loss: 0.1180 - val_accuracy: 0.9365 - val_f1_keras: 0.8860 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "\n",
      "Score 0.9134534269137403. Model not saved.\n",
      "115/115 - 4s - loss: 0.0036 - accuracy: 0.9962 - f1_keras: 0.9958 - val_loss: 0.0997 - val_accuracy: 0.9278 - val_f1_keras: 0.8748 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "\n",
      "Score 0.9164903180170356. Model not saved.\n",
      "115/115 - 4s - loss: 0.0120 - accuracy: 0.9882 - f1_keras: 0.9867 - val_loss: 0.0989 - val_accuracy: 0.9322 - val_f1_keras: 0.8817 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "\n",
      "Score 0.9082081800600277. Model not saved.\n",
      "115/115 - 4s - loss: 0.0030 - accuracy: 0.9967 - f1_keras: 0.9965 - val_loss: 0.1795 - val_accuracy: 0.9234 - val_f1_keras: 0.8673 - lr: 7.0000e-04\n",
      "Epoch 37/500\n",
      "\n",
      "Score 0.9130803146883375. Model not saved.\n",
      "115/115 - 4s - loss: 0.0032 - accuracy: 0.9964 - f1_keras: 0.9958 - val_loss: 0.1470 - val_accuracy: 0.9278 - val_f1_keras: 0.8714 - lr: 7.0000e-04\n",
      "Epoch 38/500\n",
      "\n",
      "Score 0.9176265371980115. Model not saved.\n",
      "115/115 - 4s - loss: 0.0032 - accuracy: 0.9975 - f1_keras: 0.9974 - val_loss: 0.1027 - val_accuracy: 0.9322 - val_f1_keras: 0.8811 - lr: 7.0000e-04\n",
      "Epoch 39/500\n",
      "\n",
      "Score 0.9155307056051013. Model not saved.\n",
      "115/115 - 4s - loss: 0.0021 - accuracy: 0.9975 - f1_keras: 0.9973 - val_loss: 0.1390 - val_accuracy: 0.9300 - val_f1_keras: 0.8729 - lr: 7.0000e-04\n",
      "Epoch 40/500\n",
      "\n",
      "Score 0.9225934946528593. Model saved in ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/Transformer_focal_without_features.h5.\n",
      "115/115 - 5s - loss: 5.3507e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1515 - val_accuracy: 0.9365 - val_f1_keras: 0.8867 - lr: 7.0000e-04\n",
      "Epoch 41/500\n",
      "\n",
      "Score 0.9201048951048951. Model not saved.\n",
      "115/115 - 4s - loss: 5.9361e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1519 - val_accuracy: 0.9344 - val_f1_keras: 0.8845 - lr: 7.0000e-04\n",
      "Epoch 42/500\n",
      "\n",
      "Score 0.9082081800600277. Model not saved.\n",
      "115/115 - 4s - loss: 2.2969e-04 - accuracy: 0.9995 - f1_keras: 0.9993 - val_loss: 0.1903 - val_accuracy: 0.9234 - val_f1_keras: 0.8684 - lr: 7.0000e-04\n",
      "Epoch 43/500\n",
      "\n",
      "Score 0.9147785547785547. Model not saved.\n",
      "115/115 - 4s - loss: 5.5571e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1784 - val_accuracy: 0.9300 - val_f1_keras: 0.8803 - lr: 7.0000e-04\n",
      "Epoch 44/500\n",
      "\n",
      "Score 0.9147785547785547. Model not saved.\n",
      "115/115 - 4s - loss: 3.1803e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1950 - val_accuracy: 0.9300 - val_f1_keras: 0.8803 - lr: 7.0000e-04\n",
      "Epoch 45/500\n",
      "\n",
      "Score 0.9082081800600277. Model not saved.\n",
      "115/115 - 4s - loss: 4.4445e-04 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.2102 - val_accuracy: 0.9234 - val_f1_keras: 0.8675 - lr: 7.0000e-04\n",
      "Epoch 46/500\n",
      "\n",
      "Score 0.9115136153209765. Model not saved.\n",
      "115/115 - 4s - loss: 0.0012 - accuracy: 0.9986 - f1_keras: 0.9985 - val_loss: 0.1296 - val_accuracy: 0.9278 - val_f1_keras: 0.8745 - lr: 7.0000e-04\n",
      "Epoch 47/500\n",
      "\n",
      "Score 0.9172551149737462. Model not saved.\n",
      "115/115 - 4s - loss: 4.3225e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1769 - val_accuracy: 0.9322 - val_f1_keras: 0.8752 - lr: 7.0000e-04\n",
      "Epoch 48/500\n",
      "\n",
      "Score 0.9057861462341946. Model not saved.\n",
      "115/115 - 4s - loss: 2.9313e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2209 - val_accuracy: 0.9212 - val_f1_keras: 0.8648 - lr: 7.0000e-04\n",
      "Epoch 49/500\n",
      "\n",
      "Score 0.9130803146883375. Model not saved.\n",
      "115/115 - 4s - loss: 3.2529e-04 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.1969 - val_accuracy: 0.9278 - val_f1_keras: 0.8730 - lr: 7.0000e-04\n",
      "Epoch 50/500\n",
      "\n",
      "Score 0.9102513747054202. Model not saved.\n",
      "115/115 - 4s - loss: 2.7025e-04 - accuracy: 0.9992 - f1_keras: 0.9987 - val_loss: 0.2014 - val_accuracy: 0.9256 - val_f1_keras: 0.8709 - lr: 7.0000e-04\n",
      "Epoch 51/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 2.7513e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1778 - val_accuracy: 0.9278 - val_f1_keras: 0.8728 - lr: 7.0000e-04\n",
      "Epoch 52/500\n",
      "\n",
      "Score 0.9143914204093102. Model not saved.\n",
      "115/115 - 4s - loss: 4.3475e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1629 - val_accuracy: 0.9300 - val_f1_keras: 0.8728 - lr: 7.0000e-04\n",
      "Epoch 53/500\n",
      "\n",
      "Score 0.9126999287984301. Model not saved.\n",
      "115/115 - 4s - loss: 3.2547e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1796 - val_accuracy: 0.9278 - val_f1_keras: 0.8729 - lr: 7.0000e-04\n",
      "Epoch 54/500\n",
      "\n",
      "Score 0.9106395215090868. Model not saved.\n",
      "115/115 - 4s - loss: 2.6353e-04 - accuracy: 0.9997 - f1_keras: 0.9997 - val_loss: 0.2193 - val_accuracy: 0.9256 - val_f1_keras: 0.8708 - lr: 7.0000e-04\n",
      "Epoch 55/500\n",
      "\n",
      "Score 0.9110202492211839. Model not saved.\n",
      "115/115 - 4s - loss: 4.8786e-04 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.2077 - val_accuracy: 0.9256 - val_f1_keras: 0.8698 - lr: 7.0000e-04\n",
      "Epoch 56/500\n",
      "\n",
      "Score 0.9078124549724792. Model not saved.\n",
      "115/115 - 4s - loss: 4.8859e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.1995 - val_accuracy: 0.9234 - val_f1_keras: 0.8687 - lr: 7.0000e-04\n",
      "Epoch 57/500\n",
      "\n",
      "Score 0.9189946114577425. Model not saved.\n",
      "115/115 - 4s - loss: 3.1412e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1732 - val_accuracy: 0.9344 - val_f1_keras: 0.8823 - lr: 7.0000e-04\n",
      "Epoch 58/500\n",
      "\n",
      "Score 0.904972043805739. Model not saved.\n",
      "115/115 - 4s - loss: 4.3023e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.2062 - val_accuracy: 0.9212 - val_f1_keras: 0.8666 - lr: 7.0000e-04\n",
      "Epoch 59/500\n",
      "\n",
      "Score 0.904972043805739. Model not saved.\n",
      "115/115 - 4s - loss: 5.1517e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1848 - val_accuracy: 0.9212 - val_f1_keras: 0.8666 - lr: 7.0000e-04\n",
      "Epoch 60/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 5.5689e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1713 - val_accuracy: 0.9278 - val_f1_keras: 0.8728 - lr: 7.0000e-04\n",
      "Epoch 61/500\n",
      "\n",
      "Score 0.8985744771660265. Model not saved.\n",
      "115/115 - 4s - loss: 5.5152e-04 - accuracy: 0.9992 - f1_keras: 0.9948 - val_loss: 0.2295 - val_accuracy: 0.9147 - val_f1_keras: 0.8465 - lr: 7.0000e-04\n",
      "Epoch 62/500\n",
      "\n",
      "Score 0.9102513747054202. Model not saved.\n",
      "115/115 - 4s - loss: 4.7443e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.1875 - val_accuracy: 0.9256 - val_f1_keras: 0.8709 - lr: 7.0000e-04\n",
      "Epoch 63/500\n",
      "\n",
      "Score 0.9119167352946331. Model not saved.\n",
      "115/115 - 4s - loss: 2.6986e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1891 - val_accuracy: 0.9278 - val_f1_keras: 0.8708 - lr: 7.0000e-04\n",
      "Epoch 64/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 2.6771e-04 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.2015 - val_accuracy: 0.9278 - val_f1_keras: 0.8728 - lr: 7.0000e-04\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "\n",
      "Score 0.9029629332063149. Model not saved.\n",
      "115/115 - 4s - loss: 3.8363e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2261 - val_accuracy: 0.9190 - val_f1_keras: 0.8617 - lr: 7.0000e-04\n",
      "Epoch 66/500\n",
      "\n",
      "Score 0.9078124549724792. Model not saved.\n",
      "115/115 - 4s - loss: 3.7671e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1955 - val_accuracy: 0.9234 - val_f1_keras: 0.8687 - lr: 4.9000e-04\n",
      "Epoch 67/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 2.1721e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2033 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 68/500\n",
      "\n",
      "Score 0.9078124549724792. Model not saved.\n",
      "115/115 - 4s - loss: 1.8723e-04 - accuracy: 0.9997 - f1_keras: 0.9997 - val_loss: 0.2265 - val_accuracy: 0.9234 - val_f1_keras: 0.8687 - lr: 4.9000e-04\n",
      "Epoch 69/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 3.6353e-04 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.2197 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 70/500\n",
      "\n",
      "Score 0.9102513747054202. Model not saved.\n",
      "115/115 - 4s - loss: 3.1780e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.2201 - val_accuracy: 0.9256 - val_f1_keras: 0.8707 - lr: 4.9000e-04\n",
      "Epoch 71/500\n",
      "\n",
      "Score 0.9102513747054202. Model not saved.\n",
      "115/115 - 4s - loss: 3.4607e-04 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.2157 - val_accuracy: 0.9256 - val_f1_keras: 0.8707 - lr: 4.9000e-04\n",
      "Epoch 72/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 3.3157e-04 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.2115 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 73/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 3.0823e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2191 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 74/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 2.8614e-04 - accuracy: 0.9989 - f1_keras: 0.9989 - val_loss: 0.2193 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 75/500\n",
      "\n",
      "Score 0.9078124549724792. Model not saved.\n",
      "115/115 - 4s - loss: 2.2705e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2306 - val_accuracy: 0.9234 - val_f1_keras: 0.8687 - lr: 4.9000e-04\n",
      "Epoch 76/500\n",
      "\n",
      "Score 0.9053830227743271. Model not saved.\n",
      "115/115 - 4s - loss: 3.3666e-04 - accuracy: 0.9992 - f1_keras: 0.9946 - val_loss: 0.2267 - val_accuracy: 0.9212 - val_f1_keras: 0.8648 - lr: 4.9000e-04\n",
      "Epoch 77/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 2.0824e-04 - accuracy: 0.9995 - f1_keras: 0.9993 - val_loss: 0.2250 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 78/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 3.0782e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.2273 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 79/500\n",
      "\n",
      "Score 0.9147785547785547. Model not saved.\n",
      "115/115 - 4s - loss: 3.3282e-04 - accuracy: 0.9992 - f1_keras: 0.9991 - val_loss: 0.2038 - val_accuracy: 0.9300 - val_f1_keras: 0.8733 - lr: 4.9000e-04\n",
      "Epoch 80/500\n",
      "\n",
      "Score 0.9074090153922744. Model not saved.\n",
      "115/115 - 4s - loss: 2.6899e-04 - accuracy: 0.9989 - f1_keras: 0.9988 - val_loss: 0.2155 - val_accuracy: 0.9234 - val_f1_keras: 0.8686 - lr: 4.9000e-04\n",
      "Epoch 81/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 2.3682e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2256 - val_accuracy: 0.9278 - val_f1_keras: 0.8713 - lr: 4.9000e-04\n",
      "Epoch 82/500\n",
      "\n",
      "Score 0.9078124549724792. Model not saved.\n",
      "115/115 - 4s - loss: 2.9552e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2332 - val_accuracy: 0.9234 - val_f1_keras: 0.8687 - lr: 4.9000e-04\n",
      "Epoch 83/500\n",
      "\n",
      "Score 0.9143914204093102. Model not saved.\n",
      "115/115 - 4s - loss: 2.7438e-04 - accuracy: 0.9997 - f1_keras: 0.9997 - val_loss: 0.1907 - val_accuracy: 0.9300 - val_f1_keras: 0.8746 - lr: 4.9000e-04\n",
      "Epoch 84/500\n",
      "\n",
      "Score 0.9094522144522144. Model not saved.\n",
      "115/115 - 4s - loss: 2.7839e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2190 - val_accuracy: 0.9256 - val_f1_keras: 0.8690 - lr: 4.9000e-04\n",
      "Epoch 85/500\n",
      "\n",
      "Score 0.9119167352946331. Model not saved.\n",
      "115/115 - 4s - loss: 3.3920e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.1976 - val_accuracy: 0.9278 - val_f1_keras: 0.8710 - lr: 4.9000e-04\n",
      "Epoch 86/500\n",
      "\n",
      "Score 0.9102513747054202. Model not saved.\n",
      "115/115 - 4s - loss: 2.2049e-04 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.2305 - val_accuracy: 0.9256 - val_f1_keras: 0.8707 - lr: 4.9000e-04\n",
      "Epoch 87/500\n",
      "\n",
      "Score 0.9098556576626753. Model not saved.\n",
      "115/115 - 4s - loss: 2.6431e-04 - accuracy: 0.9992 - f1_keras: 0.9990 - val_loss: 0.2304 - val_accuracy: 0.9256 - val_f1_keras: 0.8692 - lr: 4.9000e-04\n",
      "Epoch 88/500\n",
      "\n",
      "Score 0.9126999287984301. Model not saved.\n",
      "115/115 - 4s - loss: 2.6313e-04 - accuracy: 0.9995 - f1_keras: 0.9994 - val_loss: 0.2398 - val_accuracy: 0.9278 - val_f1_keras: 0.8712 - lr: 4.9000e-04\n",
      "Epoch 89/500\n",
      "\n",
      "Score 0.9123121202430445. Model not saved.\n",
      "115/115 - 4s - loss: 4.4060e-04 - accuracy: 0.9989 - f1_keras: 0.9987 - val_loss: 0.2087 - val_accuracy: 0.9278 - val_f1_keras: 0.8713 - lr: 4.9000e-04\n",
      "Epoch 90/500\n",
      "\n",
      "Score 0.915158266035459. Model not saved.\n",
      "\n",
      "Epoch 89: early stopping.\n",
      "115/115 - 4s - loss: 2.3763e-04 - accuracy: 0.9992 - f1_keras: 0.9992 - val_loss: 0.2362 - val_accuracy: 0.9300 - val_f1_keras: 0.8734 - lr: 4.9000e-04\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.96      0.92      0.94       318\n",
      "         yes       0.84      0.91      0.87       140\n",
      "\n",
      "    accuracy                           0.92       458\n",
      "   macro avg       0.90      0.92      0.91       458\n",
      "weighted avg       0.92      0.92      0.92       458\n",
      " 0.9200299404252764\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(args.model_save_path,'results.csv')):\n",
    "  results = pd.read_csv(os.path.join(args.model_save_path,'results.csv'))\n",
    "  index = results.shape[0]\n",
    "  print (results)\n",
    "else:\n",
    "  results = pd.DataFrame(columns=['config','weighted_f1','macro_f1'])\n",
    "  index = 0\n",
    "\n",
    "for model_name, model_ in all_models.items():\n",
    "    \n",
    "    for loss in ['ce','focal']:\n",
    "        \n",
    "        for use_features in [True, False]:\n",
    "            \n",
    "            if use_features == False:\n",
    "                model = model_(word_vocab_size=n_words,char_vocab_size=n_chars,wpe_vocab_size=n_subwords, n_out=n_out,max_word_char_len=args.max_word_char_len,\\\n",
    "                                             max_text_len=args.max_text_len, max_char_len=args.max_char_len,\\\n",
    "                                             n_layers=args.n_layers, n_units=args.n_units, emb_dim=args.emb_dim)\n",
    "            else:\n",
    "                model = model_(word_vocab_size=n_words,char_vocab_size=n_chars,wpe_vocab_size=n_subwords, n_out=n_out,vectorizer_shape=tfidf_shape, max_word_char_len=args.max_word_char_len,\\\n",
    "                                             max_text_len=args.max_text_len, max_char_len=args.max_char_len,\\\n",
    "                                             n_layers=args.n_layers, n_units=args.n_units, emb_dim=args.emb_dim)\n",
    "            \n",
    "            if use_features == True:\n",
    "                print (\"Running {} with features for {} loss\".format(model_name, loss))\n",
    "            else:\n",
    "                print (\"Running {} without features for {} loss\".format(model_name, loss))\n",
    "\n",
    "            print (model.summary())\n",
    "\n",
    "            if loss == 'focal':\n",
    "                model.compile(loss=models.utils.categorical_focal_loss(alpha=1), optimizer='adam', metrics=['accuracy', models.utils.f1_keras]) #binary_crossentropy\n",
    "            elif loss == 'ce':\n",
    "                model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', models.utils.f1_keras]) \n",
    "\n",
    "            lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, \\\n",
    "                                                  patience=args.lr_schedule_round, verbose=1, mode='auto', min_lr=0.000001)\n",
    "            config = {\n",
    "                  'text_max_len': args.max_text_len,\n",
    "                  'char_max_len': args.max_char_len,\n",
    "                  'word_char_max_len': args.max_word_char_len,\n",
    "                  'n_units': args.n_units,\n",
    "                  'emb_dim': args.emb_dim,\n",
    "                  'n_layers': args.n_layers,\n",
    "                  'epochs': args.epochs,\n",
    "                  \"learning_rate\": args.lr,\n",
    "                  \"model_name\": model_name,\n",
    "                  \"loss\": loss,\n",
    "                  \"use_features\": use_features\n",
    "                }\n",
    "\n",
    "            if use_features == True:\n",
    "                model_save_path = os.path.join(args.model_save_path, '{}_{}_with_features.h5'.format(model_name, config['loss']))\n",
    "            else:\n",
    "                model_save_path = os.path.join(args.model_save_path, '{}_{}_without_features.h5'.format(model_name, config['loss']))\n",
    "\n",
    "            f1callback = models.utils.F1Callback(model, [word_val_inputs, char_val_inputs, subword_val_inputs, transformer_val_inputs, val_tfidf],\\\n",
    "                                      val_outputs, \\\n",
    "                                      filename=model_save_path, \\\n",
    "                                      patience=args.early_stopping_rounds)\n",
    "\n",
    "            K.clear_session()\n",
    "\n",
    "            if _has_wandb and args.wandb_logging:\n",
    "                wandb.init(project='hate_speech_detection',config=config)\n",
    "                model.fit([word_train_inputs, char_train_inputs, subword_train_inputs, transformer_train_inputs, train_tfidf], train_outputs, \\\n",
    "                      validation_data=([word_val_inputs, char_val_inputs, subword_val_inputs, transformer_val_inputs, val_tfidf], val_outputs), \\\n",
    "                          epochs=args.epochs,batch_size=args.train_batch_size, callbacks=[lr, f1callback, WandbCallback()], verbose=2)\n",
    "            else:\n",
    "                model.fit([word_train_inputs, char_train_inputs, subword_train_inputs, transformer_train_inputs, train_tfidf], train_outputs, \\\n",
    "                      validation_data=([word_val_inputs, char_val_inputs, subword_val_inputs, transformer_val_inputs, val_tfidf], val_outputs), \\\n",
    "                          epochs=args.epochs,batch_size=args.train_batch_size, callbacks=[lr, f1callback], verbose=2)\n",
    "\n",
    "\n",
    "            model.load_weights(model_save_path)\n",
    "\n",
    "            test_pred = model.predict([word_test_inputs, char_test_inputs, subword_test_inputs, transformer_test_inputs, test_tfidf])\n",
    "\n",
    "            report = classification_report([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                                           [idx2label[i] for i in test_pred.argmax(-1)])\n",
    "\n",
    "            f1 = f1_score([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                                           [idx2label[i] for i in test_pred.argmax(-1)], average='weighted')\n",
    "\n",
    "            print (report, f1)\n",
    "            \n",
    "            results.loc[index,'config'] = str(config)\n",
    "            results.loc[index, 'weighted_f1'] = f1_score([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                                           [idx2label[i] for i in test_pred.argmax(-1)], average='weighted')\n",
    "            results.loc[index, 'macro_f1'] = f1_score([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                                           [idx2label[i] for i in test_pred.argmax(-1)], average='macro')\n",
    "            \n",
    "            index += 1\n",
    "            \n",
    "            results.to_csv(os.path.join(args.model_save_path,'results.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkLvzX4a5NOZ",
    "outputId": "315a7aa8-45f1-48ef-e266-b1c07f9d75a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CS_ELMO_without_words', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.947801</td>\n",
       "      <td>0.938759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CS_ELMO_without_words', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.930531</td>\n",
       "      <td>0.918659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HAN', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.932763</td>\n",
       "      <td>0.921351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HAN', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.941107</td>\n",
       "      <td>0.930697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CMSA', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.932640</td>\n",
       "      <td>0.921050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CMSA', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.938865</td>\n",
       "      <td>0.927987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'ce', 'use_features': True}</td>\n",
       "      <td>0.931815</td>\n",
       "      <td>0.919118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.924222</td>\n",
       "      <td>0.911536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'focal', 'use_features': True}</td>\n",
       "      <td>0.929537</td>\n",
       "      <td>0.916332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.931963</td>\n",
       "      <td>0.919455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'ce', 'use_features': True}</td>\n",
       "      <td>0.933792</td>\n",
       "      <td>0.921225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.931345</td>\n",
       "      <td>0.918065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'focal', 'use_features': True}</td>\n",
       "      <td>0.933792</td>\n",
       "      <td>0.921225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.924964</td>\n",
       "      <td>0.910721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'ce', 'use_features': True}</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.934212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.640631</td>\n",
       "      <td>0.566288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'focal', 'use_features': True}</td>\n",
       "      <td>0.935619</td>\n",
       "      <td>0.923010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.774683</td>\n",
       "      <td>0.715545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer_without_words', 'loss': 'ce', 'use_features': True}</td>\n",
       "      <td>0.925134</td>\n",
       "      <td>0.911102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': True}</td>\n",
       "      <td>0.937213</td>\n",
       "      <td>0.926701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.929378</td>\n",
       "      <td>0.915973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': True}</td>\n",
       "      <td>0.927727</td>\n",
       "      <td>0.914612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.924088</td>\n",
       "      <td>0.911202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': True}</td>\n",
       "      <td>0.936353</td>\n",
       "      <td>0.924652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': False}</td>\n",
       "      <td>0.941107</td>\n",
       "      <td>0.930697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': True}</td>\n",
       "      <td>0.923805</td>\n",
       "      <td>0.910516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': False}</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>0.906826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                     config  ...  macro_f1\n",
       "0   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CS_ELMO_without_words', 'loss': 'ce', 'use_features': False}     ...  0.938759\n",
       "1   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CS_ELMO_without_words', 'loss': 'focal', 'use_features': False}  ...  0.918659\n",
       "2   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HAN', 'loss': 'ce', 'use_features': False}                       ...  0.921351\n",
       "3   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HAN', 'loss': 'focal', 'use_features': False}                    ...  0.930697\n",
       "4   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CMSA', 'loss': 'ce', 'use_features': False}                      ...  0.921050\n",
       "5   {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'CMSA', 'loss': 'focal', 'use_features': False}                   ...  0.927987\n",
       "6   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'ce', 'use_features': True}                        ...  0.919118\n",
       "7   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'ce', 'use_features': False}                       ...  0.911536\n",
       "8   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'focal', 'use_features': True}                     ...  0.916332\n",
       "9   {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT', 'loss': 'focal', 'use_features': False}                    ...  0.919455\n",
       "10  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'ce', 'use_features': True}                  ...  0.921225\n",
       "11  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'ce', 'use_features': False}                 ...  0.918065\n",
       "12  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'focal', 'use_features': True}               ...  0.921225\n",
       "13  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer', 'loss': 'focal', 'use_features': False}              ...  0.910721\n",
       "14  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'ce', 'use_features': True}          ...  0.934212\n",
       "15  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'ce', 'use_features': False}         ...  0.566288\n",
       "16  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'focal', 'use_features': True}       ...  0.923010\n",
       "17  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_without_words', 'loss': 'focal', 'use_features': False}      ...  0.715545\n",
       "18  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'HIT_outer_without_words', 'loss': 'ce', 'use_features': True}    ...  0.911102\n",
       "19  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': True}                ...  0.926701\n",
       "20  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': False}               ...  0.915973\n",
       "21  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': True}             ...  0.914612\n",
       "22  {'text_max_len': 50, 'char_max_len': 100, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': False}            ...  0.911202\n",
       "23  {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': True}                ...  0.924652\n",
       "24  {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'ce', 'use_features': False}               ...  0.930697\n",
       "25  {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': True}             ...  0.910516\n",
       "26  {'text_max_len': 50, 'char_max_len': 200, 'word_char_max_len': 20, 'n_units': 128, 'emb_dim': 128, 'n_layers': 2, 'epochs': 500, 'learning_rate': 0.001, 'model_name': 'Transformer', 'loss': 'focal', 'use_features': False}            ...  0.906826\n",
       "\n",
       "[27 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfGv6DI_PGAB"
   },
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(args.model_save_path,'results.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjqACng3PGAB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pseudo_labelling_hate.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
