{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kCb0aehNFZC",
    "outputId": "0b816c3d-7f2d-4aa1-fb6a-5856e4be7b53"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files, drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0uJnM60PPXU",
    "outputId": "540a4f84-78a2-48f0-cf1b-adeb269584a7"
   },
   "outputs": [],
   "source": [
    "#!pip install -r ./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KWAK8Y7Gk44m"
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1CqQIcXPSkD",
    "outputId": "6ef32151-40a0-474a-87ee-e58dcb9271b5"
   },
   "outputs": [],
   "source": [
    "#!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-IqVp0DPLFa",
    "outputId": "dce8c9de-8650-4a0f-b753-3b9d43994c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: WARNING Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/victor/.netrc\n",
      "I0122 13:29:06.285727 4630683136 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0122 13:29:06.288890 4630683136 file_utils.py:57] TensorFlow version 2.2.0-rc3 available.\n",
      "/Users/victor/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    from dotenv import find_dotenv, load_dotenv\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), './drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), './drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/'))\n",
    "    \n",
    "try:\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "except:\n",
    "    sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    load_dotenv(find_dotenv())\n",
    "    wandb.login(key=os.environ['WANDB_API_KEY'])\n",
    "    from wandb.keras import WandbCallback\n",
    "    _has_wandb = True\n",
    "except:\n",
    "    _has_wandb = False\n",
    "\n",
    "import tokenizers\n",
    "from transformers import TFAutoModel, AutoTokenizer, AutoConfig, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from src import data, models\n",
    "from src.models.models import *\n",
    "\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWQ_RjDKPF_9",
    "outputId": "2ebab752-bd3f-4773-a2ba-6249c3957fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (_has_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jKb2wFKVNFZH"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(prog='Trainer',conflict_handler='resolve')\n",
    "\n",
    "#parser.add_argument('--train_data', type=str, default='./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/raw/Hate-speech-dataset/hate_speech.tsv', required=False,\n",
    "#                    help='train data')\n",
    "#parser.add_argument('--train_data', type=str, default='../data/raw/Hate-speech-dataset/hate_speech.tsv', required=False,\n",
    "#                    help='train data')\n",
    "#parser.add_argument('--data_dir', type=str, default='./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/', required=False,\n",
    "#                    help='train data')\n",
    "\n",
    "parser.add_argument('--data_dir', type=str, default='../data/', required=False,\n",
    "                    help='train data')\n",
    "\n",
    "parser.add_argument('--val_data', type=str, default=None, required=False,\n",
    "                    help='validation data')\n",
    "parser.add_argument('--test_data', type=str, default=None, required=False,\n",
    "                    help='test data')\n",
    "\n",
    "parser.add_argument('--transformer_model_pretrained_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer model pretrained path or huggingface model name')\n",
    "parser.add_argument('--transformer_config_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer config file path or huggingface model name')\n",
    "parser.add_argument('--transformer_tokenizer_path', type=str, default='roberta-base', required=False,\n",
    "                    help='transformer tokenizer file path or huggingface model name')\n",
    "\n",
    "parser.add_argument('--max_text_len', type=int, default=50, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--max_char_len', type=int, default=200, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--max_word_char_len', type=int, default=20, required=False,\n",
    "                    help='maximum length of text')\n",
    "\n",
    "parser.add_argument('--emb_dim', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--n_layers', type=int, default=2, required=False,\n",
    "                    help='maximum length of text')\n",
    "parser.add_argument('--n_units', type=int, default=128, required=False,\n",
    "                    help='maximum length of text')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=500, required=False,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--lr', type=float, default=.001, required=False,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--early_stopping_rounds', type=int, default=50, required=False,\n",
    "                    help='number of epochs for early stopping')\n",
    "parser.add_argument('--lr_schedule_round', type=int, default=30, required=False,\n",
    "                    help='number of epochs for learning rate scheduling')\n",
    "\n",
    "parser.add_argument('--train_batch_size', type=int, default=32, required=False,\n",
    "                    help='train batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=16, required=False,\n",
    "                    help='eval batch size')\n",
    "\n",
    "#parser.add_argument('--model_save_path', type=str, default='./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/models/model_hindi_sentiment/', required=False,\n",
    "#                    help='seed')\n",
    "\n",
    "parser.add_argument('--model_save_path', type=str, default='../models/hate_detection/', required=False,\n",
    "                    help='seed')\n",
    "\n",
    "parser.add_argument('--wandb_logging', type=bool, default=True, required=False,\n",
    "                    help='wandb logging needed')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=42, required=False,\n",
    "                    help='seed')\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mjTSfc1xNFZJ"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4pBa4QUENFZN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0122 13:29:13.430094 4630683136 utils.py:141] NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/raw/Hate-speech-dataset/hate_speech.tsv', \\\n",
    "#                 sep='\\t',header=None,usecols=[0,1])\n",
    "df = pd.read_csv(os.path.join(args.data_dir,'raw','Hate-speech-dataset','hate_speech.tsv'), \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'hate'\n",
    "\n",
    "hate_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "hate_val_df = df.iloc[val_index]\n",
    "hate_test_df = df.iloc[test_index]\n",
    "\n",
    "#df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/StanceDetection_CodeMixed/data.txt', \\\n",
    "#                 sep='\\t',header=None,usecols=[0,1])\n",
    "df = pd.read_csv(os.path.join(args.data_dir,'processed','StanceDetection_CodeMixed', 'data.txt'), \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'stance'\n",
    "stance_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "stance_val_df = df.iloc[val_index]\n",
    "stance_test_df = df.iloc[test_index]\n",
    "\n",
    "#df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/SarcasmDetection_CodeMixed/data.txt', \\\n",
    "#                 sep='\\t',header=None,usecols=[0,1])\n",
    "df = pd.read_csv(os.path.join(args.data_dir,'processed','SarcasmDetection_CodeMixed','data.txt'), \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'sarcasm'\n",
    "\n",
    "sarcasm_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "sarcasm_val_df = df.iloc[val_index]\n",
    "sarcasm_test_df = df.iloc[test_index]\n",
    "\n",
    "#df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/humor-detection-corpus/data.txt', \\\n",
    "#                 sep='\\t',header=None,usecols=[0,1])\n",
    "df = pd.read_csv(os.path.join(args.data_dir,'processed','humor-detection-corpus','data.txt'), \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'humor'\n",
    "\n",
    "humor_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "humor_val_df = df.iloc[val_index]\n",
    "humor_test_df = df.iloc[test_index]\n",
    "\n",
    "#df = pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/Aggression_dataset/train.txt', \\\n",
    "#                 sep='\\t',header=None,usecols=[0,1])\n",
    "#df = pd.concat([df,pd.read_csv('./drive/MyDrive/Hate_detection/Hinglish_Hate_Detection/data/processed/Aggression_dataset/val.txt', \\\n",
    "#                 sep='\\t',header=None,usecols=[0,1])],axis=0)\n",
    "df = pd.read_csv(os.path.join(args.data_dir,'processed','Aggression_dataset','train.txt'), \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])\n",
    "df = pd.concat([df,pd.read_csv(os.path.join(args.data_dir,'processed','Aggression_dataset','val.txt'), \\\n",
    "                 sep='\\t',header=None,usecols=[0,1])],axis=0)\n",
    "\n",
    "df.columns = ['text','category']\n",
    "df = df.dropna()\n",
    "df = df[df.text != '']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "for train_index, test_index in kf.split(df.text):\n",
    "    break\n",
    "\n",
    "df['type'] = 'aggression'\n",
    "\n",
    "aggression_train_df = df.iloc[train_index]\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=args.seed)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index].text):\n",
    "    break\n",
    "\n",
    "aggression_val_df = df.iloc[val_index]\n",
    "aggression_test_df = df.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrfukC0aNFZW",
    "outputId": "71577480-9f40-44be-b66c-46e6a77af157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37059, 3) (4632, 3) (4634, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([hate_train_df,stance_train_df,sarcasm_train_df,humor_train_df,aggression_train_df],axis=0)\n",
    "val_df = pd.concat([hate_val_df,stance_val_df,sarcasm_val_df,humor_val_df,aggression_val_df],axis=0)\n",
    "test_df = pd.concat([hate_test_df,stance_test_df,sarcasm_test_df,humor_test_df,aggression_test_df],axis=0)\n",
    "\n",
    "print (train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "GWR12E5dNFZZ",
    "outputId": "db3dd43e-a436-4b60-911a-d197b0b7c50e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>Great stand by father but the guy needs a burial???</td>\n",
       "      <td>CAG</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>PAYTM is a unscrupulous vendor see my experience : I Paid through Paytm ( order no. 2532865244) payment of Rs.590/- towards my CESC Electricity Bill for CESC Consumer ID-60000208248 on January 6 morning, but after 5 days , it is still showing PROCESSING, I have raised numerous tickets, but no one is there to reply me. Is it a reliable service ? or birth of mistrust ? I wonder how our Central Government is promoting such scrap?</td>\n",
       "      <td>CAG</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>Just saw this news\\n* read 1dt comment. \\n*smiled  and  thought  it must be ndtv or hindustantimes. \\n* scrolled  up and checked.. Ndtv. \\n* laughed and said saeed.. Ye madharchod nhi sudrege. \\n*moved on.</td>\n",
       "      <td>CAG</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>U guys can crack jokes but can't take the cridibility of Indian armed force.... already war with Pak is one sided ie 4 -0 ....</td>\n",
       "      <td>CAG</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>These media fellows have time to dissect one's social media profile,but none of spineless jokers has the guts to question as to why and how a convict like Lalu Yadav is roaming outside and running a Government.</td>\n",
       "      <td>CAG</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "5995  Great stand by father but the guy needs a burial???                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "5996  PAYTM is a unscrupulous vendor see my experience : I Paid through Paytm ( order no. 2532865244) payment of Rs.590/- towards my CESC Electricity Bill for CESC Consumer ID-60000208248 on January 6 morning, but after 5 days , it is still showing PROCESSING, I have raised numerous tickets, but no one is there to reply me. Is it a reliable service ? or birth of mistrust ? I wonder how our Central Government is promoting such scrap?   \n",
       "5998  Just saw this news\\n* read 1dt comment. \\n*smiled  and  thought  it must be ndtv or hindustantimes. \\n* scrolled  up and checked.. Ndtv. \\n* laughed and said saeed.. Ye madharchod nhi sudrege. \\n*moved on.                                                                                                                                                                                                                                    \n",
       "5999  U guys can crack jokes but can't take the cridibility of Indian armed force.... already war with Pak is one sided ie 4 -0 ....                                                                                                                                                                                                                                                                                                                   \n",
       "6000  These media fellows have time to dissect one's social media profile,but none of spineless jokers has the guts to question as to why and how a convict like Lalu Yadav is roaming outside and running a Government.                                                                                                                                                                                                                               \n",
       "\n",
       "     category        type  \n",
       "5995  CAG      aggression  \n",
       "5996  CAG      aggression  \n",
       "5998  CAG      aggression  \n",
       "5999  CAG      aggression  \n",
       "6000  CAG      aggression  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JFvdZlgPF_9",
    "outputId": "939b5174-07a0-4fb0-adcf-9051fdf6ad8d"
   },
   "outputs": [],
   "source": [
    "train_df.text = train_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "val_df.text = val_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "test_df.text = test_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "\n",
    "train_df = train_df[train_df.text != '']\n",
    "val_df = val_df[val_df.text != '']\n",
    "test_df = test_df[test_df.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "76PsUvCk3rUZ"
   },
   "outputs": [],
   "source": [
    "hate_train_df.text = hate_train_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "hate_val_df.text = hate_val_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "hate_test_df.text = hate_test_df.text.apply(lambda x: data.preprocessing.clean_tweets(x))\n",
    "\n",
    "hate_train_df = hate_train_df[hate_train_df.text != '']\n",
    "hate_val_df = hate_val_df[hate_val_df.text != '']\n",
    "hate_test_df = hate_test_df[hate_test_df.text != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecLj6RWuVfdt",
    "outputId": "1a4831a6-335d-42b5-dd9a-732f2cf4ea0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37048.000000\n",
       "mean     122.742307  \n",
       "std      175.689032  \n",
       "min      1.000000    \n",
       "25%      57.000000   \n",
       "50%      89.000000   \n",
       "75%      131.000000  \n",
       "max      6867.000000 \n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text.apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oAyiM1jPF_9",
    "outputId": "051d8dd7-1243-4a7b-dfb4-ef2bde3f92e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37048.000000\n",
       "mean     22.966746   \n",
       "std      32.452479   \n",
       "min      1.000000    \n",
       "25%      11.000000   \n",
       "50%      17.000000   \n",
       "75%      25.000000   \n",
       "max      1124.000000 \n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text.apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "V7YDv6wZNFZc"
   },
   "outputs": [],
   "source": [
    "model_save_dir = args.model_save_path\n",
    "\n",
    "try:\n",
    "    os.makedirs(model_save_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5_NT3X9cuiO",
    "outputId": "36176967-5313-4cec-ea1a-becbc9916b17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     2316\n",
       "yes    1342\n",
       "on     2   \n",
       "n      1   \n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_train_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ihJokEqR3rUa"
   },
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.category.str.contains('yes|no')]\n",
    "#val_df = val_df[val_df.category.str.contains('yes|no')]\n",
    "#test_df = test_df[test_df.category.str.contains('yes|no')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HogRodQIkh1O"
   },
   "outputs": [],
   "source": [
    "hate_train_df = hate_train_df[hate_train_df.category.str.contains('yes|no')]\n",
    "hate_val_df = hate_val_df[hate_val_df.category.str.contains('yes|no')]\n",
    "hate_test_df = hate_test_df[hate_test_df.category.str.contains('yes|no')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SQFNK1zLdyT7",
    "outputId": "f7ea9eb9-0629-4ac6-a8ad-9ba2a1525364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/hate_detection/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ch7LVidpBNtE"
   },
   "outputs": [],
   "source": [
    "hate_train_df.category, label2idx = data.data_utils.convert_categorical_label_to_int(hate_train_df.category.values, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))\n",
    "\n",
    "hate_val_df.category, _ = data.data_utils.convert_categorical_label_to_int(hate_val_df.category.values, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))\n",
    "\n",
    "hate_test_df.category, _ = data.data_utils.convert_categorical_label_to_int(hate_test_df.category.values, \\\n",
    "                                                         save_path=os.path.join(model_save_dir,'label2idx.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBiGShWtBRKW",
    "outputId": "645294b1-0a48-4f71-8488-77c05e629180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 0, 'yes': 1}\n"
     ]
    }
   ],
   "source": [
    "print (label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8DPcQeSiBSaV"
   },
   "outputs": [],
   "source": [
    "idx2label = {i:w for (w,i) in label2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isoI88xCPF_-"
   },
   "source": [
    "### Learn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "t4zgghyWPF_-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0122 13:29:19.011621 4630683136 tokenization_utils.py:420] Model name '../models/hate_detection/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../models/hate_detection/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0122 13:29:19.014900 4630683136 tokenization_utils.py:449] Didn't find file ../models/hate_detection/added_tokens.json. We won't load it.\n",
      "I0122 13:29:19.015823 4630683136 tokenization_utils.py:449] Didn't find file ../models/hate_detection/special_tokens_map.json. We won't load it.\n",
      "I0122 13:29:19.016721 4630683136 tokenization_utils.py:449] Didn't find file ../models/hate_detection/tokenizer_config.json. We won't load it.\n",
      "I0122 13:29:19.019674 4630683136 tokenization_utils.py:502] loading file ../models/hate_detection/vocab.txt\n",
      "I0122 13:29:19.025113 4630683136 tokenization_utils.py:502] loading file None\n",
      "I0122 13:29:19.026390 4630683136 tokenization_utils.py:502] loading file None\n",
      "I0122 13:29:19.031069 4630683136 tokenization_utils.py:502] loading file None\n"
     ]
    }
   ],
   "source": [
    "#data.custom_tokenizers.custom_wp_tokenizer(hate_train_df.text.values, args.model_save_path, args.model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nBWTdHbAPF_-"
   },
   "outputs": [],
   "source": [
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000, split=' ',oov_token=1)\n",
    "char_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, split='',oov_token=1)\n",
    "\n",
    "word_tokenizer.fit_on_texts(hate_train_df.text.values)\n",
    "char_tokenizer.fit_on_texts(hate_train_df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gSp7NirPF_-"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "O8cHPep9PF_-"
   },
   "outputs": [],
   "source": [
    "n_words = len(word_tokenizer.word_index)+1\n",
    "n_chars = len(char_tokenizer.word_index)+1\n",
    "n_subwords = tokenizer.vocab_size\n",
    "tfidf_shape = None #train_tfidf.shape[1]\n",
    "n_out = len(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "RHYdrtMGlDe7"
   },
   "outputs": [],
   "source": [
    "model = HAN(word_vocab_size=n_words,char_vocab_size=n_chars,wpe_vocab_size=n_subwords, n_out=n_out,max_word_char_len=args.max_word_char_len,\\\n",
    "                                             max_text_len=args.max_text_len, max_char_len=args.max_char_len,\\\n",
    "                                             n_layers=args.n_layers, n_units=args.n_units, emb_dim=args.emb_dim)\n",
    "\n",
    "model.load_weights(os.path.join(args.model_save_path,'HAN_ce_without_features.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_weights(text, model, class_idx):\n",
    "    #all_texts = [\" \".join(text.split()[:i]) for i in range(len(text.split())+1)][1:]\n",
    "    all_texts = [text]*(len(text.split())+1)\n",
    "    word_train_inputs = word_tokenizer.texts_to_sequences(all_texts)\n",
    "    word_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_train_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "    subword_train_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                            for text in all_texts])\n",
    "\n",
    "    char_train_inputs = char_tokenizer.texts_to_sequences(all_texts)\n",
    "    char_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_train_inputs, maxlen=args.max_char_len)\n",
    "    \n",
    "    for i in range(len(text.split())):\n",
    "        #subword_train_inputs_ = subword_train_inputs.copy()\n",
    "        subword_train_inputs[i,i,:] = np.array([0]*subword_train_inputs.shape[-1])\n",
    "        \n",
    "    #print (subword_train_inputs[-1])\n",
    "    probs = model.predict([word_train_inputs,char_train_inputs,subword_train_inputs,\\\n",
    "                                      char_train_inputs])\n",
    "    \n",
    "    probs = probs[-1] - probs\n",
    "    #class_idx = probs.argmax(-1)[-1]\n",
    "    \n",
    "    return probs[:-1,class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4573/4573 [1:47:51<00:00,  1.42s/it]  \n"
     ]
    }
   ],
   "source": [
    "word_causal_weights = []\n",
    "words = []\n",
    "\n",
    "causal_weights_df = pd.DataFrame()\n",
    "\n",
    "for text in tqdm(pd.concat([hate_train_df, hate_val_df, hate_test_df], axis=0).text.values):\n",
    "    text = \" \".join(text.split()[:args.max_text_len])\n",
    "    weights = get_conditional_weights(text,model,1)\n",
    "    for i, word in enumerate(text.split()):\n",
    "        words.append(word)\n",
    "        word_causal_weights.append(weights[i])\n",
    "        \n",
    "causal_weights_df['word'] = words\n",
    "causal_weights_df['Hate_yes'] = word_causal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Hate_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11803</th>\n",
       "      <td>he</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78936</th>\n",
       "      <td>he</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87416</th>\n",
       "      <td>i</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11802</th>\n",
       "      <td>gyei.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>i</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51787</th>\n",
       "      <td>karte</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65378</th>\n",
       "      <td>rape</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78934</th>\n",
       "      <td>nikal</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78935</th>\n",
       "      <td>gyei.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53188</th>\n",
       "      <td>i</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  Hate_yes\n",
       "11803  he     1.0     \n",
       "78936  he     1.0     \n",
       "87416  i      1.0     \n",
       "11802  gyei.  1.0     \n",
       "9242   i      1.0     \n",
       "51787  karte  1.0     \n",
       "65378  rape   1.0     \n",
       "78934  nikal  1.0     \n",
       "78935  gyei.  1.0     \n",
       "53188  i      1.0     "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#causal_weights_df = causal_weights_df.groupby(['word'])['Hate_yes'].mean().reset_index(drop=False)\n",
    "causal_weights_df.sort_values(['Hate_yes'],ascending=[False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_weights_df.to_csv(os.path.join(args.model_save_path,'word_weights.csv'),index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37867/37867 [00:19<00:00, 1915.39it/s]\n",
      "100%|██████████| 37867/37867 [00:20<00:00, 1823.07it/s]\n"
     ]
    }
   ],
   "source": [
    "transformer_train_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(train_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_train_inputs = word_tokenizer.texts_to_sequences(train_df.text.values)\n",
    "word_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_train_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_train_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(train_df.text.values)])\n",
    "\n",
    "char_train_inputs = char_tokenizer.texts_to_sequences(train_df.text.values)\n",
    "char_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_train_inputs, maxlen=args.max_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Y1y38uBX47Pe"
   },
   "outputs": [],
   "source": [
    "probs = model.predict([word_train_inputs,char_train_inputs,subword_train_inputs,\\\n",
    "                                      transformer_train_inputs])\n",
    "train_df['category_prob'] = probs.max(-1)\n",
    "train_df['category'] = probs.argmax(-1)\n",
    "\n",
    "train_df = train_df[train_df.category_prob > .95]\n",
    "train_df = train_df.sort_values(['category_prob'],ascending=[False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctor  sab sahi me ke phd (in hate politics) wale. bhai padhe likhe ho fir kyu ye sab baate karte ho. tum bas bowling  khelo, aur maje lo. pic.twitter.com/fkqubqstw</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poore desh me patel obc me aate hain sirf gujrat ko chor kar may be, ye manuwadiyon bramanwadi kabhi aapko aarackchan nahi denge ye to jis obc ko mila hai usse bhi nafrat karte hain ye khoon aur chamdi ka frak karne waale bharmhanwadi kisi ke sage nahi hain</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hate it jab test ata ho phr bh acha na ho -.-</td>\n",
       "      <td>1</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ek dil ek jaan akal ke imitihaan kal kp ke rape aur sawing between two legs after rape honge justified due to cinematic liberty aaj toh rajput ki ho gayi shaan urdu ki gulaam</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aise logo se sakht nafrat karta hu jo caste ko naam ke sath jod ke chaude hote h but real me vo piddu hote h</td>\n",
       "      <td>1</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                text  \\\n",
       "2  doctor  sab sahi me ke phd (in hate politics) wale. bhai padhe likhe ho fir kyu ye sab baate karte ho. tum bas bowling  khelo, aur maje lo. pic.twitter.com/fkqubqstw                                                                                               \n",
       "3  poore desh me patel obc me aate hain sirf gujrat ko chor kar may be, ye manuwadiyon bramanwadi kabhi aapko aarackchan nahi denge ye to jis obc ko mila hai usse bhi nafrat karte hain ye khoon aur chamdi ka frak karne waale bharmhanwadi kisi ke sage nahi hain   \n",
       "5  hate it jab test ata ho phr bh acha na ho -.-                                                                                                                                                                                                                       \n",
       "6  ek dil ek jaan akal ke imitihaan kal kp ke rape aur sawing between two legs after rape honge justified due to cinematic liberty aaj toh rajput ki ho gayi shaan urdu ki gulaam                                                                                      \n",
       "7  aise logo se sakht nafrat karta hu jo caste ko naam ke sath jod ke chaude hote h but real me vo piddu hote h                                                                                                                                                        \n",
       "\n",
       "  category  type  \n",
       "2  0        hate  \n",
       "3  0        hate  \n",
       "5  1        hate  \n",
       "6  0        hate  \n",
       "7  1        hate  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/457 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▌     | 206/457 [00:00<00:00, 2049.96it/s]\u001b[A\n",
      "100%|██████████| 457/457 [00:00<00:00, 2023.65it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/457 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▍     | 204/457 [00:00<00:00, 2038.98it/s]\u001b[A\n",
      "100%|██████████| 457/457 [00:00<00:00, 1942.12it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/458 [00:00<?, ?it/s]\u001b[A\n",
      " 45%|████▌     | 208/458 [00:00<00:00, 2079.63it/s]\u001b[A\n",
      "100%|██████████| 458/458 [00:00<00:00, 1963.43it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/458 [00:00<?, ?it/s]\u001b[A\n",
      " 43%|████▎     | 195/458 [00:00<00:00, 1949.06it/s]\u001b[A\n",
      "100%|██████████| 458/458 [00:00<00:00, 1788.73it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4737 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 211/4737 [00:00<00:02, 2107.09it/s]\u001b[A\n",
      "  9%|▊         | 414/4737 [00:00<00:02, 2081.45it/s]\u001b[A\n",
      " 13%|█▎        | 598/4737 [00:00<00:02, 1999.83it/s]\u001b[A\n",
      " 17%|█▋        | 823/4737 [00:00<00:01, 2064.59it/s]\u001b[A\n",
      " 21%|██▏       | 1008/4737 [00:00<00:01, 1994.64it/s]\u001b[A\n",
      " 26%|██▌       | 1224/4737 [00:00<00:01, 2041.23it/s]\u001b[A\n",
      " 30%|███       | 1437/4737 [00:00<00:01, 2065.04it/s]\u001b[A\n",
      " 36%|███▋      | 1723/4737 [00:00<00:01, 2252.85it/s]\u001b[A\n",
      " 41%|████      | 1942/4737 [00:00<00:01, 1995.84it/s]\u001b[A\n",
      " 45%|████▌     | 2143/4737 [00:01<00:01, 1897.89it/s]\u001b[A\n",
      " 49%|████▉     | 2335/4737 [00:01<00:01, 1852.45it/s]\u001b[A\n",
      " 53%|█████▎    | 2522/4737 [00:01<00:01, 1758.12it/s]\u001b[A\n",
      " 57%|█████▋    | 2700/4737 [00:01<00:01, 1724.24it/s]\u001b[A\n",
      " 61%|██████    | 2875/4737 [00:01<00:01, 1730.48it/s]\u001b[A\n",
      " 64%|██████▍   | 3050/4737 [00:01<00:01, 1660.64it/s]\u001b[A\n",
      " 68%|██████▊   | 3222/4737 [00:01<00:00, 1676.70it/s]\u001b[A\n",
      " 72%|███████▏  | 3391/4737 [00:01<00:00, 1624.74it/s]\u001b[A\n",
      " 76%|███████▌  | 3582/4737 [00:01<00:00, 1687.63it/s]\u001b[A\n",
      " 79%|███████▉  | 3753/4737 [00:02<00:00, 1574.83it/s]\u001b[A\n",
      " 83%|████████▎ | 3934/4737 [00:02<00:00, 1636.42it/s]\u001b[A\n",
      " 87%|████████▋ | 4121/4737 [00:02<00:00, 1692.02it/s]\u001b[A\n",
      " 91%|█████████ | 4293/4737 [00:02<00:00, 1672.08it/s]\u001b[A\n",
      " 94%|█████████▍| 4462/4737 [00:02<00:00, 1631.21it/s]\u001b[A\n",
      "100%|██████████| 4737/4737 [00:02<00:00, 1793.03it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/4737 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 202/4737 [00:00<00:02, 2017.00it/s]\u001b[A\n",
      "  9%|▊         | 403/4737 [00:00<00:02, 2014.71it/s]\u001b[A\n",
      " 12%|█▏        | 585/4737 [00:00<00:02, 1946.74it/s]\u001b[A\n",
      " 17%|█▋        | 812/4737 [00:00<00:01, 2030.61it/s]\u001b[A\n",
      " 21%|██        | 994/4737 [00:00<00:01, 1961.35it/s]\u001b[A\n",
      " 25%|██▌       | 1199/4737 [00:00<00:01, 1986.47it/s]\u001b[A\n",
      " 30%|██▉       | 1411/4737 [00:00<00:01, 2023.17it/s]\u001b[A\n",
      " 36%|███▌      | 1683/4737 [00:00<00:01, 2191.60it/s]\u001b[A\n",
      " 40%|████      | 1896/4737 [00:00<00:01, 1975.86it/s]\u001b[A\n",
      " 44%|████▍     | 2093/4737 [00:01<00:01, 1793.19it/s]\u001b[A\n",
      " 48%|████▊     | 2275/4737 [00:01<00:01, 1765.14it/s]\u001b[A\n",
      " 52%|█████▏    | 2454/4737 [00:01<00:01, 1624.50it/s]\u001b[A\n",
      " 56%|█████▌    | 2637/4737 [00:01<00:01, 1648.48it/s]\u001b[A\n",
      " 59%|█████▉    | 2805/4737 [00:01<00:01, 1618.33it/s]\u001b[A\n",
      " 63%|██████▎   | 2983/4737 [00:01<00:01, 1660.80it/s]\u001b[A\n",
      " 67%|██████▋   | 3151/4737 [00:01<00:01, 1534.28it/s]\u001b[A\n",
      " 70%|██████▉   | 3308/4737 [00:01<00:00, 1521.99it/s]\u001b[A\n",
      " 73%|███████▎  | 3463/4737 [00:01<00:00, 1529.18it/s]\u001b[A\n",
      " 76%|███████▋  | 3618/4737 [00:02<00:00, 1501.11it/s]\u001b[A\n",
      " 80%|███████▉  | 3770/4737 [00:02<00:00, 1448.94it/s]\u001b[A\n",
      " 83%|████████▎ | 3917/4737 [00:02<00:00, 1435.53it/s]\u001b[A\n",
      " 86%|████████▋ | 4089/4737 [00:02<00:00, 1508.58it/s]\u001b[A\n",
      " 90%|████████▉ | 4242/4737 [00:02<00:00, 1479.24it/s]\u001b[A\n",
      " 93%|█████████▎| 4392/4737 [00:02<00:00, 1482.00it/s]\u001b[A\n",
      " 96%|█████████▋| 4562/4737 [00:02<00:00, 1498.46it/s]\u001b[A\n",
      "100%|██████████| 4737/4737 [00:02<00:00, 1685.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37867, 200) (37867, 50, 20) (37867, 50) (37867, 200) (37867, 2)\n",
      "(457, 200) (457, 50, 20) (457, 50) (457, 200) (457, 2)\n",
      "(458, 200) (458, 50, 20) (458, 50) (458, 200) (458, 2)\n",
      "(4737, 200) (4737, 50, 20) (4737, 50) (4737, 200)\n"
     ]
    }
   ],
   "source": [
    "transformer_train_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(train_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_train_inputs = word_tokenizer.texts_to_sequences(train_df.text.values)\n",
    "word_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_train_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_train_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(train_df.text.values)])\n",
    "\n",
    "char_train_inputs = char_tokenizer.texts_to_sequences(train_df.text.values)\n",
    "char_train_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_train_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "train_outputs = data.data_utils.compute_output_arrays(train_df, 'category')\n",
    "\n",
    "transformer_val_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(hate_val_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_val_inputs = word_tokenizer.texts_to_sequences(hate_val_df.text.values)\n",
    "word_val_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_val_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_val_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(hate_val_df.text.values)])\n",
    "\n",
    "char_val_inputs = char_tokenizer.texts_to_sequences(hate_val_df.text.values)\n",
    "char_val_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_val_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "val_outputs = data.data_utils.compute_output_arrays(hate_val_df, 'category')\n",
    "\n",
    "transformer_test_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(hate_test_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "word_test_inputs = word_tokenizer.texts_to_sequences(hate_test_df.text.values)\n",
    "word_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(word_test_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "subword_test_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(hate_test_df.text.values)])\n",
    "\n",
    "char_test_inputs = char_tokenizer.texts_to_sequences(hate_test_df.text.values)\n",
    "char_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(char_test_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "full_transformer_test_inputs, _, _ = data.data_utils.compute_transformer_input_arrays(test_df, 'text', tokenizer, args.max_char_len)\n",
    "\n",
    "full_word_test_inputs = word_tokenizer.texts_to_sequences(test_df.text.values)\n",
    "full_word_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(full_word_test_inputs, maxlen=args.max_text_len)\n",
    "\n",
    "full_subword_test_inputs = np.asarray([data.data_utils.subword_tokenization(text, char_tokenizer, args.max_text_len, args.max_word_char_len) \\\n",
    "                        for text in tqdm(test_df.text.values)])\n",
    "\n",
    "full_char_test_inputs = char_tokenizer.texts_to_sequences(test_df.text.values)\n",
    "full_char_test_inputs = tf.keras.preprocessing.sequence.pad_sequences(full_char_test_inputs, maxlen=args.max_char_len)\n",
    "\n",
    "test_outputs = data.data_utils.compute_output_arrays(hate_test_df, 'category')\n",
    "\n",
    "train_outputs = tf.keras.utils.to_categorical(train_outputs, \\\n",
    "                                                    num_classes=len(label2idx))\n",
    "val_outputs = tf.keras.utils.to_categorical(val_outputs, \\\n",
    "                                                    num_classes=len(label2idx))\n",
    "test_outputs = tf.keras.utils.to_categorical(test_outputs, \\\n",
    "                                                    num_classes=len(label2idx))\n",
    "\n",
    "print (transformer_train_inputs.shape, subword_train_inputs.shape, word_train_inputs.shape, char_train_inputs.shape, \\\n",
    "       train_outputs.shape)\n",
    "print (transformer_val_inputs.shape, subword_val_inputs.shape, word_val_inputs.shape, char_val_inputs.shape, \\\n",
    "       val_outputs.shape)\n",
    "print (transformer_test_inputs.shape, subword_test_inputs.shape, word_test_inputs.shape, char_test_inputs.shape, \\\n",
    "       test_outputs.shape)\n",
    "print (full_transformer_test_inputs.shape, full_subword_test_inputs.shape, full_word_test_inputs.shape, \\\n",
    "       full_char_test_inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TQGocn-PF_-",
    "outputId": "88caa3da-7fc5-4559-eed0-e0b26fc4dd17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1184/1184 [==============================] - ETA: 0s - loss: 0.5258 - accuracy: 0.7473 - f1_keras: 0.6114\n",
      "Score 0.1814272030651341. Model saved in ../models/hate_detection/pseudo_labelling_transformer.h5.\n",
      "1184/1184 [==============================] - 450s 380ms/step - loss: 0.5258 - accuracy: 0.7473 - f1_keras: 0.6114 - val_loss: 1.7445 - val_accuracy: 0.1816 - val_f1_keras: 0.1678 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "  38/1184 [..............................] - ETA: 7:02 - loss: 0.3654 - accuracy: 0.8388 - f1_keras: 0.7694"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-77c350f95d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model.fit([word_train_inputs, char_train_inputs, subword_train_inputs, transformer_train_inputs], train_outputs, \\\n\u001b[1;32m     20\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_val_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_val_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubword_val_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_val_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                           epochs=args.epochs,batch_size=args.train_batch_size, callbacks=[lr, f1callback], verbose=1)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(word_vocab_size=n_words,char_vocab_size=n_chars,wpe_vocab_size=n_subwords, n_out=n_out,max_word_char_len=args.max_word_char_len,\\\n",
    "                                             max_text_len=args.max_text_len, max_char_len=args.max_char_len,\\\n",
    "                                             n_layers=args.n_layers, n_units=args.n_units, emb_dim=args.emb_dim)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', models.utils.f1_keras]) \n",
    "\n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, \\\n",
    "                                                  patience=args.lr_schedule_round, verbose=1, mode='auto', min_lr=0.000001)\n",
    "\n",
    "model_save_path = os.path.join(args.model_save_path,'pseudo_labelling_transformer.h5')\n",
    "\n",
    "f1callback = models.utils.F1Callback(model, [word_val_inputs, char_val_inputs, subword_val_inputs, transformer_val_inputs],\\\n",
    "                                      val_outputs, \\\n",
    "                                      filename=model_save_path, \\\n",
    "                                      patience=args.early_stopping_rounds)\n",
    "\n",
    "K.clear_session()\n",
    "    \n",
    "model.fit([word_train_inputs, char_train_inputs, subword_train_inputs, transformer_train_inputs], train_outputs, \\\n",
    "                      validation_data=([word_val_inputs, char_val_inputs, subword_val_inputs, transformer_val_inputs], val_outputs), \\\n",
    "                          epochs=args.epochs,batch_size=args.train_batch_size, callbacks=[lr, f1callback], verbose=1)\n",
    "\n",
    "model.load_weights(model_save_path)\n",
    "\n",
    "test_pred = model.predict([word_test_inputs, char_test_inputs, subword_test_inputs, transformer_test_inputs])\n",
    "\n",
    "report = classification_report([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                               [idx2label[i] for i in test_pred.argmax(-1)])\n",
    "\n",
    "f1 = f1_score([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                               [idx2label[i] for i in test_pred.argmax(-1)], average='weighted')\n",
    "\n",
    "f1_macro = f1_score([idx2label[i] for i in test_outputs.argmax(-1)], \\\n",
    "                               [idx2label[i] for i in test_pred.argmax(-1)], average='macro')\n",
    "\n",
    "print (report, f1, f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjqACng3PGAB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pseudo_labelling_hate.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
